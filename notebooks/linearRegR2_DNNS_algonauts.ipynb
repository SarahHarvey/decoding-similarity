{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 1. IMPORT LIBRARIES & SET GLOBAL VARS #\n",
    "#########################################\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# import gdown\n",
    "\n",
    "# Threshold used for selecting reliable voxels.\n",
    "NCSNR_THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import RidgeCV  # using RidgeCV with a fixed alpha\n",
    "from sklearn.metrics import r2_score as r2_score_sklearn\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Algonauts data\n",
    "\n",
    "with open('../algonauts_brain_data_joint_images_8subjects.pkl', 'rb') as f:\n",
    "    brainData = pickle.load(f)\n",
    "\n",
    "shared_images = np.load('../algonauts_joint_images_8subjects.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'maxvit_t',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_models = models.list_models(module=torchvision.models)\n",
    "avail_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_large done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_tiny done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet169 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet201 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_l done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_m done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool1\n",
      "conv2\n",
      "conv3\n",
      "maxpool2\n",
      "inception3a\n",
      "inception3b\n",
      "maxpool3\n",
      "inception4a\n",
      "inception4b\n",
      "inception4c\n",
      "inception4d\n",
      "inception4e\n",
      "maxpool4\n",
      "inception5a\n",
      "inception5b\n",
      "aux1\n",
      "aux2\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "googlenet done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "Conv2d_1a_3x3\n",
      "Conv2d_2a_3x3\n",
      "Conv2d_2b_3x3\n",
      "maxpool1\n",
      "Conv2d_3b_1x1\n",
      "Conv2d_4a_3x3\n",
      "maxpool2\n",
      "Mixed_5b\n",
      "Mixed_5c\n",
      "Mixed_5d\n",
      "Mixed_6a\n",
      "Mixed_6b\n",
      "Mixed_6c\n",
      "Mixed_6d\n",
      "Mixed_6e\n",
      "AuxLogits\n",
      "Mixed_7a\n",
      "Mixed_7b\n",
      "Mixed_7c\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "inception_v3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /dev/shm/nix-build-py-torch-2.2.2.drv-0/nixbld1/spack-stage-py-torch-2.2.2-mrga1lajkybghn1l6sc83lyspvbakfzk/spack-src/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "blocks\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "maxvit_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_75 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v2 done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_large done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_400mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_128gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_400mf done\n",
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet34 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet50 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_32x8d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_64x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext50_32x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x0_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_0 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x2_0 done\n",
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet101_2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet50_2 done\n"
     ]
    }
   ],
   "source": [
    "# Extract representations resulting from probe inputs image_data for a list of models, and save them individually (takes a long time)\n",
    "\n",
    "import extract_internal_reps\n",
    "\n",
    "# internal_reps = []\n",
    "# model_2nds = []\n",
    "repDict = {}\n",
    "\n",
    "model_names = avail_models[2:] #[\"alexnet\"]\n",
    "weights = 'random' #'first'\n",
    "# image_data = test_image_data[0:1000,:,:,:]\n",
    "image_data = shared_images\n",
    "batch_size = 32\n",
    "\n",
    "for model in model_names:  #avail_models:\n",
    "    repDict = {}\n",
    "    if model == 'vit_h_14':\n",
    "        continue\n",
    "    else:\n",
    "        repDict[model + '_random'] = extract_internal_reps.get_model_activations(model, weights, image_data, batch_size=32, saverep = True, filename = 'algonauts_shared_images_random_weights')\n",
    "        print(model + \" done\")\n",
    "\n",
    "    del repDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "densenet121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../reps/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_algonauts_shared_images.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 11\u001b[0m         new_reps \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         repDict\u001b[38;5;241m.\u001b[39mupdate(new_reps)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(model_name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load some saved representations\n",
    "\n",
    "repDict = {}\n",
    "\n",
    "model_names = [\"alexnet\", \"resnet50\", \"vit_b_16\"] #avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "for model_name in model_names:\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(model_name)\n",
    "\n",
    "# model_names, _ = zip(*repDict.items())\n",
    "model_names = [value for value in repDict.keys()]\n",
    "# internal_reps = [value[0] for value in repDict.values()]\n",
    "\n",
    "\n",
    "layer_names = []\n",
    "internal_reps = []\n",
    "layer_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose subject and target brain region\n",
    "\n",
    "\n",
    "# areas = ['lh_FFA-1', 'rh_FFA-1', 'lh_FFA-2', 'rh_FFA-2']\n",
    "# areas = ['lh_all-faces', 'rh_all-faces']\n",
    "# areas = ['lh_all-words', 'rh_all-words']\n",
    "# areas = [\"lh_V1v\", \"rh_V1v\", \"lh_V1d\", \"rh_V1d\"]\n",
    "# areas = ['lh_OWFA', 'rh_OWFA']\n",
    "# areas_both_hemi = [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"] \n",
    "\n",
    "subjind = 0\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"FFA-1\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "brain_target = brainData[0][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    brain_target = np.append(brain_target, brainData[0][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(872, 541875),\n",
       " (872, 193600),\n",
       " (872, 193600),\n",
       " (872, 46656),\n",
       " (872, 139968),\n",
       " (872, 139968),\n",
       " (872, 32448),\n",
       " (872, 64896),\n",
       " (872, 64896),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 9216),\n",
       " (872, 9216),\n",
       " (872, 9216),\n",
       " (872, 1000),\n",
       " (872, 541875),\n",
       " (872, 802816),\n",
       " (872, 802816),\n",
       " (872, 802816),\n",
       " (872, 200704),\n",
       " (872, 802816),\n",
       " (872, 401408),\n",
       " (872, 200704),\n",
       " (872, 100352),\n",
       " (872, 2048),\n",
       " (872, 1000),\n",
       " (872, 541875),\n",
       " (872, 150528),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 1000)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure representations are flattened\n",
    "\n",
    "internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "\n",
    "[internal_rep.shape for internal_rep in internal_reps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and test sets\n",
    "\n",
    "internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "brain_target_train = brain_target[0:436]\n",
    "brain_target_test = brain_target[436:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ridge regression and compute R^2 score for the particular model layers and particular brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "resnet50\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "vit_b_16\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "\n",
    "# clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexnet': ['feature.12', 0.17273218158595455],\n",
       " 'resnet50': ['layer3', 0.1730730631140099],\n",
       " 'vit_b_16': ['encoder', 0.14308709373932468]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a9de1160>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjind = 0\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"FFA-2\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "brain_target = brainData[subjind][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    brain_target = np.append(brain_target, brainData[subjind][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjind = 7\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"FFA-1\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "extra_brain = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "extra_brain = brainData[subjind][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    extra_brain = np.append(extra_brain, brainData[subjind][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestScores = {}\n",
    "filename = 'bestScores_FFA-2_both'\n",
    "with open(filename + '.pkl', 'wb') as f:\n",
    "    pickle.dump(bestScores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded alexnet\n",
      "alexnet prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "alexnet done\n",
      "loaded convnext_base\n",
      "convnext_base prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_base done\n",
      "loaded convnext_large\n",
      "convnext_large prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_large done\n",
      "loaded convnext_small\n",
      "convnext_small prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_small done\n",
      "loaded convnext_tiny\n",
      "convnext_tiny prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_tiny done\n",
      "loaded densenet121\n",
      "densenet121 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "densenet121 done\n",
      "loaded densenet161\n",
      "densenet161 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "densenet161 done\n",
      "loaded densenet169\n",
      "densenet169 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "densenet169 done\n",
      "loaded densenet201\n",
      "densenet201 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "densenet201 done\n",
      "loaded efficientnet_b0\n",
      "efficientnet_b0 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b0 done\n",
      "loaded efficientnet_b1\n",
      "efficientnet_b1 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b1 done\n",
      "loaded efficientnet_b2\n",
      "efficientnet_b2 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b2 done\n",
      "loaded efficientnet_b3\n",
      "efficientnet_b3 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b3 done\n",
      "loaded efficientnet_b4\n",
      "efficientnet_b4 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b4 done\n",
      "loaded efficientnet_b5\n",
      "efficientnet_b5 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b5 done\n",
      "loaded efficientnet_b6\n",
      "efficientnet_b6 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "efficientnet_b6 done\n",
      "loaded efficientnet_b7\n",
      "efficientnet_b7 prepped\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# A better way\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = avail_models\n",
    "model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(\"loaded \" + model_name)\n",
    "    # repDict[model_name] = {\"FFA-1\": extra_brain}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded wide_resnet101_2\n",
      "wide_resnet101_2 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "wide_resnet101_2 done\n",
      "loaded wide_resnet50_2\n",
      "wide_resnet50_2 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "wide_resnet50_2 done\n"
     ]
    }
   ],
   "source": [
    "# A better way\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images_random_weights.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(\"loaded \" + model_name)\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name + '_random'] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexnet': ['feature.12', 0.17273298223512806],\n",
       " 'convnext_base': ['avgpool', 0.11783628194091364],\n",
       " 'convnext_large': ['feature.4', 0.11760911126680311],\n",
       " 'convnext_small': ['feature.6', 0.12468310663079814],\n",
       " 'convnext_tiny': ['feature.6', 0.14512439504105765],\n",
       " 'densenet121': ['feature.denseblock4', 0.18827860445022881],\n",
       " 'densenet161': ['feature.denseblock4', 0.193282643088318],\n",
       " 'densenet169': ['feature.denseblock4', 0.18863635005498122],\n",
       " 'densenet201': ['feature.denseblock4', 0.17935394536822025],\n",
       " 'efficientnet_b0': ['feature.6', 0.16144281962878057],\n",
       " 'efficientnet_b1': ['feature.6', 0.16017359370183298],\n",
       " 'efficientnet_b2': ['feature.6', 0.1731910176299413],\n",
       " 'efficientnet_b3': ['feature.6', 0.1750512137026707],\n",
       " 'efficientnet_b4': ['feature.6', 0.1696341206767184],\n",
       " 'efficientnet_b5': ['feature.6', 0.18191511410186317],\n",
       " 'efficientnet_b6': ['feature.6', 0.1754231684860041],\n",
       " 'efficientnet_b7': ['feature.6', 0.17795740812785316],\n",
       " 'efficientnet_v2_l': ['feature.6', 0.17430840385746135],\n",
       " 'efficientnet_v2_m': ['feature.6', 0.16258618863365923],\n",
       " 'efficientnet_v2_s': ['feature.7', 0.15528412261110938],\n",
       " 'googlenet': ['inception5a', 0.18087135113091396],\n",
       " 'inception_v3': ['Mixed_7a', 0.1943134170443355],\n",
       " 'maxvit_t': ['block.2', 0.1539933257147688],\n",
       " 'mnasnet0_5': ['layer.12', 0.18498330546815145],\n",
       " 'mnasnet0_75': ['layer.13', 0.16670990311600448],\n",
       " 'mnasnet1_0': ['layer.12', 0.16141711497027372],\n",
       " 'mnasnet1_3': ['layer.12', 0.14766083782637063],\n",
       " 'mobilenet_v2': ['feature.15', 0.18522155340902902],\n",
       " 'mobilenet_v3_large': ['avgpool', 0.16826355401062287],\n",
       " 'mobilenet_v3_small': ['avgpool', 0.1673561767573082],\n",
       " 'regnet_x_16gf': ['block3', 0.16353813877913312],\n",
       " 'regnet_x_1_6gf': ['block3', 0.17560728938783013],\n",
       " 'regnet_x_32gf': ['block3', 0.15887759927952183],\n",
       " 'regnet_x_3_2gf': ['block3', 0.17128771335987297],\n",
       " 'regnet_x_400mf': ['block4', 0.17377359648507223],\n",
       " 'regnet_x_800mf': ['block4', 0.169735778934318],\n",
       " 'regnet_x_8gf': ['block3', 0.16640789273980328],\n",
       " 'regnet_y_128gf': ['fc', 0.17691555142530002],\n",
       " 'regnet_y_16gf': ['block3', 0.17351420334416195],\n",
       " 'regnet_y_1_6gf': ['block3', 0.17695373688542973],\n",
       " 'regnet_y_32gf': ['block3', 0.16905419438129052],\n",
       " 'regnet_y_3_2gf': ['block3', 0.17359192540988555],\n",
       " 'regnet_y_400mf': ['block4', 0.16947750497575106],\n",
       " 'regnet_y_800mf': ['block3', 0.1737981061093409],\n",
       " 'regnet_y_8gf': ['block3', 0.1669799323126854],\n",
       " 'resnet101': ['layer3', 0.17933880345590403],\n",
       " 'resnet152': ['layer3', 0.18281925790728756],\n",
       " 'resnet18': ['layer4', 0.16627013680064612],\n",
       " 'resnet34': ['layer4', 0.16604832354936092],\n",
       " 'resnet50': ['layer3', 0.17307305628382205],\n",
       " 'resnext101_32x8d': ['layer3', 0.17449947305543723],\n",
       " 'resnext101_64x4d': ['layer3', 0.14646179155596092],\n",
       " 'resnext50_32x4d': ['layer3', 0.1724574292181022],\n",
       " 'shufflenet_v2_x0_5': ['stage4', 0.16936093935317134],\n",
       " 'shufflenet_v2_x1_0': ['stage4', 0.18600757321625666],\n",
       " 'shufflenet_v2_x1_5': ['stage4', 0.18511938633908628],\n",
       " 'shufflenet_v2_x2_0': ['stage4', 0.17756123944264096],\n",
       " 'squeezenet1_0': ['classifier', 0.17454599512762145],\n",
       " 'squeezenet1_1': ['classifier', 0.16914608016094307],\n",
       " 'swin_b': ['norm', 0.12487885132590985],\n",
       " 'swin_s': ['feature.6', 0.13156151487988194],\n",
       " 'swin_t': ['feature.6', 0.174251693641862],\n",
       " 'swin_v2_b': ['feature.5', 0.14368363471550624],\n",
       " 'swin_v2_s': ['feature.5', 0.15179308726335528],\n",
       " 'swin_v2_t': ['feature.6', 0.19969256111018538],\n",
       " 'vgg11': ['feature.16', 0.17607349269139133],\n",
       " 'vgg11_bn': ['feature.22', 0.17054129958088446],\n",
       " 'vgg13': ['feature.20', 0.17364390190496126],\n",
       " 'vgg13_bn': ['feature.28', 0.17216960648532365],\n",
       " 'vgg16': ['feature.24', 0.1780464625441111],\n",
       " 'vgg16_bn': ['feature.34', 0.17267274425917378],\n",
       " 'vgg19': ['feature.32', 0.17567136678968884],\n",
       " 'vgg19_bn': ['feature.43', 0.17828818312092287],\n",
       " 'vit_b_16': ['encoder', 0.1430872555965293],\n",
       " 'vit_b_32': ['encoder_layer_9', 0.16873805436279415],\n",
       " 'vit_l_16': ['encoder', 0.1491176202301482],\n",
       " 'vit_l_32': ['encoder_layer_14', 0.17265302544103975],\n",
       " 'wide_resnet101_2': ['layer3', 0.17909120459869876],\n",
       " 'wide_resnet50_2': ['layer3', 0.16942749521066655],\n",
       " 'alexnet_random': ['feature.12', 0.03407387881762681],\n",
       " 'convnext_base_random': ['feature.6', 0.009259463478093167],\n",
       " 'convnext_large_random': ['feature.3', 0.010046229294165644],\n",
       " 'convnext_small_random': ['feature.1', 0.008616471832413267],\n",
       " 'convnext_tiny_random': ['feature.0', 0.008344907579972239],\n",
       " 'densenet121_random': ['classifier', 0.02407066539891257],\n",
       " 'densenet161_random': ['classifier', 0.016443299730458473],\n",
       " 'densenet169_random': ['classifier', 0.017385677011662028],\n",
       " 'densenet201_random': ['classifier', 0.024133823844448916],\n",
       " 'efficientnet_b0_random': ['feature.0', 0.005498716636463175],\n",
       " 'efficientnet_b1_random': ['feature.0', 0.004370589936203907],\n",
       " 'efficientnet_b2_random': ['feature.0', 0.004258963339584407],\n",
       " 'efficientnet_b3_random': ['feature.0', 0.006268601587081565],\n",
       " 'efficientnet_b4_random': ['feature.0', 0.005233758979504873],\n",
       " 'efficientnet_b5_random': ['feature.4', 0.008061812486130419],\n",
       " 'efficientnet_b6_random': ['feature.1', 0.004888294902270232],\n",
       " 'efficientnet_b7_random': ['feature.0', 0.003834968679561365],\n",
       " 'efficientnet_v2_l_random': ['feature.0', 0.004940293228125702],\n",
       " 'efficientnet_v2_m_random': ['feature.0', 0.006694348697181767],\n",
       " 'efficientnet_v2_s_random': ['feature.1', 0.0075264312047364085],\n",
       " 'googlenet_random': ['inception4a', 0.023071049323615714],\n",
       " 'inception_v3_random': ['Conv2d_1a_3x3', 0.007639005631315526],\n",
       " 'maxvit_t_random': ['block.1', 0.01843440387391855],\n",
       " 'mnasnet0_5_random': ['layer.3', 0.006365780371509026],\n",
       " 'mnasnet0_75_random': ['layer.11', 0.013943334732106918],\n",
       " 'mnasnet1_0_random': ['layer.3', 0.014237732394383616],\n",
       " 'mnasnet1_3_random': ['layer.11', 0.011056749887365144],\n",
       " 'mobilenet_v2_random': ['feature.9', 0.012656276020988547],\n",
       " 'mobilenet_v3_large_random': ['feature.1', 0.001566196328455545],\n",
       " 'mobilenet_v3_small_random': ['feature.1', 0.007881676740433627],\n",
       " 'regnet_x_16gf_random': ['fc', 0.016093520470773753],\n",
       " 'regnet_x_1_6gf_random': ['stem', 0.008150477290317686],\n",
       " 'regnet_x_32gf_random': ['avgpool', 0.016810751454747818],\n",
       " 'regnet_x_3_2gf_random': ['avgpool', 0.010083969090066537],\n",
       " 'regnet_x_400mf_random': ['avgpool', 0.014326706516889092],\n",
       " 'regnet_x_800mf_random': ['avgpool', 0.012968244889457059],\n",
       " 'regnet_x_8gf_random': ['avgpool', 0.011418441226686154],\n",
       " 'regnet_y_128gf_random': ['avgpool', 0.013945047674673835],\n",
       " 'regnet_y_16gf_random': ['fc', 0.012955988391260043],\n",
       " 'regnet_y_1_6gf_random': ['stem', 0.009218110420586659],\n",
       " 'regnet_y_32gf_random': ['avgpool', 0.013886672622773683],\n",
       " 'regnet_y_3_2gf_random': ['avgpool', 0.012069724577714686],\n",
       " 'regnet_y_400mf_random': ['stem', 0.008334746967847403],\n",
       " 'regnet_y_800mf_random': ['avgpool', 0.014138298552097002],\n",
       " 'regnet_y_8gf_random': ['fc', 0.00925184215557767],\n",
       " 'resnet101_random': ['maxpool', 0.015467302202759825],\n",
       " 'resnet152_random': ['maxpool', 0.014717385169738324],\n",
       " 'resnet18_random': ['avgpool', 0.01705098100834783],\n",
       " 'resnet34_random': ['layer1', 0.014853308051995568],\n",
       " 'resnet50_random': ['maxpool', 0.014652628554299064],\n",
       " 'resnext101_32x8d_random': ['avgpool', 0.020303183063625337],\n",
       " 'resnext101_64x4d_random': ['fc', 0.021095727738610758],\n",
       " 'resnext50_32x4d_random': ['fc', 0.017237355571443205],\n",
       " 'shufflenet_v2_x0_5_random': ['maxpool', 0.00879900263713011],\n",
       " 'shufflenet_v2_x1_0_random': ['maxpool', 0.013634071578468358],\n",
       " 'shufflenet_v2_x1_5_random': ['maxpool', 0.012952668981811603],\n",
       " 'shufflenet_v2_x2_0_random': ['stage2', 0.01621777239918873],\n",
       " 'squeezenet1_0_random': ['classifier', 0.01641228399867107],\n",
       " 'squeezenet1_1_random': ['classifier', 0.02036798290404524],\n",
       " 'swin_b_random': ['avgpool', 0.014710286629665927],\n",
       " 'swin_s_random': ['head', 0.014914955467976006],\n",
       " 'swin_t_random': ['avgpool', 0.013318825629007904],\n",
       " 'swin_v2_b_random': ['feature.5', 0.01717431017510084],\n",
       " 'swin_v2_s_random': ['feature.5', 0.020395849526422257],\n",
       " 'swin_v2_t_random': ['permute', 0.019122924056357307],\n",
       " 'vgg11_random': ['classifier', 0.023535145810587157],\n",
       " 'vgg11_bn_random': ['feature.28', 0.018436810295533325],\n",
       " 'vgg13_random': ['feature.24', 0.014089645198466803],\n",
       " 'vgg13_bn_random': ['feature.32', 0.027038383217136808],\n",
       " 'vgg16_random': ['feature.30', 0.0308760079357178],\n",
       " 'vgg16_bn_random': ['feature.25', 0.013229995110176465],\n",
       " 'vgg19_random': ['feature.32', 0.021277150784305584],\n",
       " 'vgg19_bn_random': ['feature.41', 0.023365568313425372],\n",
       " 'vit_b_16_random': ['encoder', 0.013352065247176262],\n",
       " 'vit_b_32_random': ['encoder_layer_11', 0.012214699176662647],\n",
       " 'vit_l_16_random': ['encoder', 0.016984683022888573],\n",
       " 'vit_l_32_random': ['encoder_layer_11', 0.01245735231067364],\n",
       " 'wide_resnet101_2_random': ['maxpool', 0.010648475006613395],\n",
       " 'wide_resnet50_2_random': ['maxpool', 0.012243154916225733],\n",
       " 'subject_1': ['FFA-1', 0.2206819231607037],\n",
       " 'subject_2': ['FFA-1', 0.20032597392137727],\n",
       " 'subject_3': ['FFA-1', 0.19319909621208514],\n",
       " 'subject_4': ['FFA-1', 0.2063503761115546],\n",
       " 'subject_5': ['FFA-1', 0.16276399287905066],\n",
       " 'subject_6': ['FFA-1', 0.17256906632963911],\n",
       " 'subject_7': ['FFA-1', 0.16931504889401405]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bestScores_FFA-1_both.pkl', 'rb') as f:\n",
    "    saved_scores = pickle.load(f)\n",
    "\n",
    "saved_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in saved_scores.keys():\n",
    "    scores.append(saved_scores[model][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of models')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores[:79],20)\n",
    "plt.hist(scores[79:-7],20)\n",
    "plt.plot( [scores[-1], scores[-1]],[0, 20])\n",
    "plt.plot( [scores[-2], scores[-2]],[0, 20])\n",
    "plt.plot( [scores[-3], scores[-3]],[0, 20])\n",
    "plt.plot( [scores[-4], scores[-4]],[0, 20])\n",
    "plt.plot( [scores[-5], scores[-5]],[0, 20])\n",
    "plt.plot( [scores[-6], scores[-6]],[0, 20])\n",
    "plt.plot( [scores[-7], scores[-7]],[0, 20])\n",
    "\n",
    "\n",
    "plt.xlabel('R^2 score')\n",
    "plt.ylabel('Number of models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1552c157be00>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(scores),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - noise ceiling normalization\n",
    "# - normalize by how well humans predict each other "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
