{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 1. IMPORT LIBRARIES & SET GLOBAL VARS #\n",
    "#########################################\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# import gdown\n",
    "\n",
    "# Threshold used for selecting reliable voxels.\n",
    "NCSNR_THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import RidgeCV  # using RidgeCV with a fixed alpha\n",
    "from sklearn.metrics import r2_score as r2_score_sklearn\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Algonauts data\n",
    "\n",
    "with open('../algonauts_brain_data_joint_images_8subjects.pkl', 'rb') as f:\n",
    "    brainData = pickle.load(f)\n",
    "\n",
    "shared_images = np.load('../algonauts_joint_images_8subjects.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'maxvit_t',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_models = models.list_models(module=torchvision.models)\n",
    "avail_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_large done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_tiny done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet169 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet201 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_l done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_m done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool1\n",
      "conv2\n",
      "conv3\n",
      "maxpool2\n",
      "inception3a\n",
      "inception3b\n",
      "maxpool3\n",
      "inception4a\n",
      "inception4b\n",
      "inception4c\n",
      "inception4d\n",
      "inception4e\n",
      "maxpool4\n",
      "inception5a\n",
      "inception5b\n",
      "aux1\n",
      "aux2\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "googlenet done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "Conv2d_1a_3x3\n",
      "Conv2d_2a_3x3\n",
      "Conv2d_2b_3x3\n",
      "maxpool1\n",
      "Conv2d_3b_1x1\n",
      "Conv2d_4a_3x3\n",
      "maxpool2\n",
      "Mixed_5b\n",
      "Mixed_5c\n",
      "Mixed_5d\n",
      "Mixed_6a\n",
      "Mixed_6b\n",
      "Mixed_6c\n",
      "Mixed_6d\n",
      "Mixed_6e\n",
      "AuxLogits\n",
      "Mixed_7a\n",
      "Mixed_7b\n",
      "Mixed_7c\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "inception_v3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /dev/shm/nix-build-py-torch-2.2.2.drv-0/nixbld1/spack-stage-py-torch-2.2.2-mrga1lajkybghn1l6sc83lyspvbakfzk/spack-src/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "blocks\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "maxvit_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_75 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v2 done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_large done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_400mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_128gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_400mf done\n",
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet34 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet50 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_32x8d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_64x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext50_32x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x0_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_0 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x2_0 done\n",
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet101_2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet50_2 done\n"
     ]
    }
   ],
   "source": [
    "# Extract representations resulting from probe inputs image_data for a list of models, and save them individually (takes a long time)\n",
    "\n",
    "import extract_internal_reps\n",
    "\n",
    "# internal_reps = []\n",
    "# model_2nds = []\n",
    "repDict = {}\n",
    "\n",
    "model_names = avail_models[2:] #[\"alexnet\"]\n",
    "weights = 'random' #'first'\n",
    "# image_data = test_image_data[0:1000,:,:,:]\n",
    "image_data = shared_images\n",
    "batch_size = 32\n",
    "\n",
    "for model in model_names:  #avail_models:\n",
    "    repDict = {}\n",
    "    if model == 'vit_h_14':\n",
    "        continue\n",
    "    else:\n",
    "        repDict[model + '_random'] = extract_internal_reps.get_model_activations(model, weights, image_data, batch_size=32, saverep = True, filename = 'algonauts_shared_images_random_weights')\n",
    "        print(model + \" done\")\n",
    "\n",
    "    del repDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose target subject/brain region\n",
    "\n",
    "subjind = 0\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"V1d\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "brain_target = brainData[subjind][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    brain_target = np.append(brain_target, brainData[subjind][area],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 991)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestScores = {}\n",
    "filename = 'bestScores_V1d_both'\n",
    "with open(filename + '.pkl', 'wb') as f:\n",
    "    pickle.dump(bestScores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded alexnet\n",
      "alexnet prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "alexnet done\n",
      "loaded convnext_base\n",
      "convnext_base prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_base done\n",
      "loaded convnext_large\n",
      "convnext_large prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_large done\n",
      "loaded convnext_small\n",
      "convnext_small prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_small done\n",
      "loaded convnext_tiny\n",
      "convnext_tiny prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "convnext_tiny done\n",
      "loaded densenet121\n",
      "densenet121 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "densenet121 done\n"
     ]
    }
   ],
   "source": [
    "# Compute fit R^2 for the trained models\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(\"loaded \" + model_name)\n",
    "    # repDict[model_name] = {\"FFA-2\": extra_brain}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute fit R^2 for randomly initialized models\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images_random_weights.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(\"loaded \" + model_name)\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name + '_random'] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional humans\n",
    "\n",
    "model_names = ['human_1', 'human_2', 'human_3', 'human_4', 'human_5', 'human_6', 'human_7']\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "subjind = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"V1d\"] \n",
    "count = 0\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    # with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "    #     new_reps = pickle.load(f)\n",
    "    #     repDict.update(new_reps)\n",
    "    #     print(\"loaded \" + model_name)\n",
    "    \n",
    "    areas = []\n",
    "    extra_brain = []\n",
    "    for area in areas_to_append:\n",
    "        if hemisphere == \"both\":\n",
    "            areas.append('lh_' + area)\n",
    "            areas.append('rh_' + area)\n",
    "        elif hemisphere == \"lh\":\n",
    "            areas.append('lh_' + area)\n",
    "        elif hemisphere == \"rh\":\n",
    "            areas.append('rh_' + area)\n",
    "    \n",
    "    # all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "    \n",
    "    extra_brain = brainData[subjind[count]][areas[0]]\n",
    "    for area in areas[1:]:\n",
    "        extra_brain = np.append(extra_brain, brainData[subjind[count]][area],axis=1)\n",
    "    \n",
    "    repDict[model_name] = {areas_to_append[0]: extra_brain}\n",
    "    \n",
    "\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexnet': ['classifier', 0.2629882785376796],\n",
       " 'convnext_base': ['avgpool', 0.20189465608140975],\n",
       " 'convnext_large': ['avgpool', 0.19688836166867849],\n",
       " 'convnext_small': ['feature.6', 0.20189262833062696],\n",
       " 'convnext_tiny': ['feature.6', 0.23974698294737992],\n",
       " 'densenet121': ['feature.denseblock4', 0.2691980772713266],\n",
       " 'densenet161': ['feature.denseblock4', 0.27916894657055513],\n",
       " 'densenet169': ['feature.denseblock4', 0.2775763623196283],\n",
       " 'densenet201': ['feature.denseblock4', 0.2670617465939616],\n",
       " 'efficientnet_b0': ['feature.7', 0.239987701231372],\n",
       " 'efficientnet_b1': ['feature.6', 0.23702287786825627],\n",
       " 'efficientnet_b2': ['feature.6', 0.2567503755453765],\n",
       " 'efficientnet_b3': ['feature.6', 0.2628202763734767],\n",
       " 'efficientnet_b4': ['feature.6', 0.2578763339603632],\n",
       " 'efficientnet_b5': ['feature.6', 0.26394790142513613],\n",
       " 'efficientnet_b6': ['feature.6', 0.25539344103897266],\n",
       " 'efficientnet_b7': ['feature.6', 0.26204935506916205],\n",
       " 'efficientnet_v2_l': ['feature.6', 0.2572181050715184],\n",
       " 'efficientnet_v2_m': ['feature.6', 0.2399475852856073],\n",
       " 'efficientnet_v2_s': ['feature.6', 0.25137975349385233],\n",
       " 'googlenet': ['inception5a', 0.2651748681735172],\n",
       " 'inception_v3': ['Mixed_7a', 0.27714894032591175],\n",
       " 'maxvit_t': ['block.2', 0.2272918584937834],\n",
       " 'mnasnet0_5': ['layer.12', 0.278833144231872],\n",
       " 'mnasnet0_75': ['layer.13', 0.2583107459494418],\n",
       " 'mnasnet1_0': ['layer.14', 0.2588739262789357],\n",
       " 'mnasnet1_3': ['layer.12', 0.24822202078113678],\n",
       " 'mobilenet_v2': ['feature.15', 0.26771675163514885],\n",
       " 'mobilenet_v3_large': ['avgpool', 0.2584322204510143],\n",
       " 'mobilenet_v3_small': ['avgpool', 0.25777166963358383],\n",
       " 'regnet_x_16gf': ['avgpool', 0.25617604999205973],\n",
       " 'regnet_x_1_6gf': ['avgpool', 0.2639776880602783],\n",
       " 'regnet_x_32gf': ['block3', 0.24059164441037273],\n",
       " 'regnet_x_3_2gf': ['block3', 0.2669182093326899],\n",
       " 'regnet_x_400mf': ['avgpool', 0.26231933490641873],\n",
       " 'regnet_x_800mf': ['block4', 0.263707977560491],\n",
       " 'regnet_x_8gf': ['block3', 0.25145068396824777],\n",
       " 'regnet_y_128gf': ['avgpool', 0.2706346062418057],\n",
       " 'regnet_y_16gf': ['block3', 0.2543102965008835],\n",
       " 'regnet_y_1_6gf': ['block3', 0.26251496452462064],\n",
       " 'regnet_y_32gf': ['block3', 0.24984171847510825],\n",
       " 'regnet_y_3_2gf': ['block3', 0.26763355162906355],\n",
       " 'regnet_y_400mf': ['block4', 0.2629137696349022],\n",
       " 'regnet_y_800mf': ['avgpool', 0.26487088073356024],\n",
       " 'regnet_y_8gf': ['block3', 0.2534730921650614],\n",
       " 'resnet101': ['layer3', 0.26499595272135407],\n",
       " 'resnet152': ['layer3', 0.2674052401224031],\n",
       " 'resnet18': ['avgpool', 0.25019893350593436],\n",
       " 'resnet34': ['avgpool', 0.25746311989096127],\n",
       " 'resnet50': ['avgpool', 0.26049313974519606],\n",
       " 'resnext101_32x8d': ['layer3', 0.25616944283321036],\n",
       " 'resnext101_64x4d': ['fc', 0.23968262366007853],\n",
       " 'resnext50_32x4d': ['layer3', 0.2530512576668873],\n",
       " 'shufflenet_v2_x0_5': ['stage4', 0.2520091758884589],\n",
       " 'shufflenet_v2_x1_0': ['stage4', 0.28228916312910934],\n",
       " 'shufflenet_v2_x1_5': ['stage4', 0.2839736472026665],\n",
       " 'shufflenet_v2_x2_0': ['stage4', 0.27672667295387343],\n",
       " 'squeezenet1_0': ['classifier', 0.2715461497096333],\n",
       " 'squeezenet1_1': ['classifier', 0.26684287900595144],\n",
       " 'swin_b': ['norm', 0.22761458721429859],\n",
       " 'swin_s': ['permute', 0.2304776642159341],\n",
       " 'swin_t': ['feature.6', 0.2696441005493462],\n",
       " 'swin_v2_b': ['feature.5', 0.23325884105478603],\n",
       " 'swin_v2_s': ['feature.5', 0.24327595944158897],\n",
       " 'swin_v2_t': ['feature.6', 0.29133156005955085],\n",
       " 'vgg11': ['feature.20', 0.26326234679068516],\n",
       " 'vgg11_bn': ['feature.28', 0.2596503881029231],\n",
       " 'vgg13': ['classifier', 0.25658600914468027],\n",
       " 'vgg13_bn': ['classifier', 0.2640697323676886],\n",
       " 'vgg16': ['feature.26', 0.2564375123431831],\n",
       " 'vgg16_bn': ['classifier', 0.2535679367799638],\n",
       " 'vgg19': ['feature.32', 0.2613765732271949],\n",
       " 'vgg19_bn': ['classifier', 0.2589271445605697],\n",
       " 'vit_b_16': ['encoder', 0.22819716638743126],\n",
       " 'vit_b_32': ['encoder_layer_9', 0.252840371120677],\n",
       " 'vit_l_16': ['encoder_layer_19', 0.23547037020788728],\n",
       " 'vit_l_32': ['encoder_layer_14', 0.25475296360689464],\n",
       " 'wide_resnet101_2': ['layer3', 0.2567339025766729],\n",
       " 'wide_resnet50_2': ['layer3', 0.24694797798677737],\n",
       " 'alexnet_random': ['feature.12', 0.05058673126946859],\n",
       " 'convnext_base_random': ['feature.7', 0.019326479585386778],\n",
       " 'convnext_large_random': ['feature.0', 0.02009290475686095],\n",
       " 'convnext_small_random': ['feature.1', 0.018137465034642728],\n",
       " 'convnext_tiny_random': ['feature.0', 0.018198978659816942],\n",
       " 'densenet121_random': ['classifier', 0.03775905927748176],\n",
       " 'densenet161_random': ['classifier', 0.03808840189798059],\n",
       " 'densenet169_random': ['classifier', 0.03631083448870181],\n",
       " 'densenet201_random': ['classifier', 0.03473009275442309],\n",
       " 'efficientnet_b0_random': ['feature.0', 0.01695784864561403],\n",
       " 'efficientnet_b1_random': ['feature.0', 0.01630437182288438],\n",
       " 'efficientnet_b2_random': ['feature.0', 0.01633930512813337],\n",
       " 'efficientnet_b3_random': ['feature.0', 0.018335832840170485],\n",
       " 'efficientnet_b4_random': ['feature.0', 0.017133790250257592],\n",
       " 'efficientnet_b5_random': ['feature.0', 0.018506109008222155],\n",
       " 'efficientnet_b6_random': ['feature.1', 0.01647179166346059],\n",
       " 'efficientnet_b7_random': ['feature.0', 0.016395754587462542],\n",
       " 'efficientnet_v2_l_random': ['feature.0', 0.017125378124095564],\n",
       " 'efficientnet_v2_m_random': ['feature.0', 0.018358513748798176],\n",
       " 'efficientnet_v2_s_random': ['feature.1', 0.017600430095680623],\n",
       " 'googlenet_random': ['inception4a', 0.05053410072709057],\n",
       " 'inception_v3_random': ['Conv2d_1a_3x3', 0.01934263529876826],\n",
       " 'maxvit_t_random': ['block.1', 0.03306258568970084],\n",
       " 'mnasnet0_5_random': ['layer.10', 0.018359576907930247],\n",
       " 'mnasnet0_75_random': ['layer.1', 0.01941873467124116],\n",
       " 'mnasnet1_0_random': ['layer.3', 0.02278038615096092],\n",
       " 'mnasnet1_3_random': ['layer.11', 0.018959271003932857],\n",
       " 'mobilenet_v2_random': ['feature.9', 0.022364744844426496],\n",
       " 'mobilenet_v3_large_random': ['feature.3', 0.013302512782477955],\n",
       " 'mobilenet_v3_small_random': ['feature.0', 0.017046072948379464],\n",
       " 'regnet_x_16gf_random': ['fc', 0.02233646061495948],\n",
       " 'regnet_x_1_6gf_random': ['stem', 0.019305329279459358],\n",
       " 'regnet_x_32gf_random': ['avgpool', 0.02107135664960215],\n",
       " 'regnet_x_3_2gf_random': ['stem', 0.018087589610991873],\n",
       " 'regnet_x_400mf_random': ['block1', 0.018519701345191243],\n",
       " 'regnet_x_800mf_random': ['avgpool', 0.019130902669994558],\n",
       " 'regnet_x_8gf_random': ['stem', 0.017266568216202486],\n",
       " 'regnet_y_128gf_random': ['fc', 0.01900974042717625],\n",
       " 'regnet_y_16gf_random': ['fc', 0.02062044076949257],\n",
       " 'regnet_y_1_6gf_random': ['stem', 0.018745240092147338],\n",
       " 'regnet_y_32gf_random': ['avgpool', 0.021398038901295692],\n",
       " 'regnet_y_3_2gf_random': ['block1', 0.01924443141021792],\n",
       " 'regnet_y_400mf_random': ['stem', 0.019541514567715662],\n",
       " 'regnet_y_800mf_random': ['stem', 0.01926117676177039],\n",
       " 'regnet_y_8gf_random': ['stem', 0.01573276967738293],\n",
       " 'resnet101_random': ['maxpool', 0.024087257195491586],\n",
       " 'resnet152_random': ['maxpool', 0.02493734953900087],\n",
       " 'resnet18_random': ['avgpool', 0.024252104043605283],\n",
       " 'resnet34_random': ['layer1', 0.022527066431891018],\n",
       " 'resnet50_random': ['maxpool', 0.02303591122365745],\n",
       " 'resnext101_32x8d_random': ['fc', 0.029106226403351757],\n",
       " 'resnext101_64x4d_random': ['avgpool', 0.03547686571343092],\n",
       " 'resnext50_32x4d_random': ['avgpool', 0.030982134809569142],\n",
       " 'shufflenet_v2_x0_5_random': ['maxpool', 0.01832878777754293],\n",
       " 'shufflenet_v2_x1_0_random': ['maxpool', 0.02297009394301916],\n",
       " 'shufflenet_v2_x1_5_random': ['maxpool', 0.021985907337821388],\n",
       " 'shufflenet_v2_x2_0_random': ['maxpool', 0.02395973914155517],\n",
       " 'squeezenet1_0_random': ['classifier', 0.03020193472686787],\n",
       " 'squeezenet1_1_random': ['classifier', 0.03847585509361963],\n",
       " 'swin_b_random': ['feature.7', 0.02250667383482191],\n",
       " 'swin_s_random': ['feature.5', 0.02104792993394897],\n",
       " 'swin_t_random': ['head', 0.02252721609082271],\n",
       " 'swin_v2_b_random': ['feature.6', 0.029007241198464404],\n",
       " 'swin_v2_s_random': ['feature.5', 0.035111852292813064],\n",
       " 'swin_v2_t_random': ['permute', 0.03484799276431302],\n",
       " 'vgg11_random': ['classifier', 0.040418115905821755],\n",
       " 'vgg11_bn_random': ['classifier', 0.03155579681205384],\n",
       " 'vgg13_random': ['feature.24', 0.025525484170989005],\n",
       " 'vgg13_bn_random': ['classifier', 0.03929961391963105],\n",
       " 'vgg16_random': ['feature.30', 0.043756366520880496],\n",
       " 'vgg16_bn_random': ['feature.41', 0.028718863762422057],\n",
       " 'vgg19_random': ['feature.36', 0.03146425470345678],\n",
       " 'vgg19_bn_random': ['feature.52', 0.02983491417888536],\n",
       " 'vit_b_16_random': ['encoder', 0.023680444032095273],\n",
       " 'vit_b_32_random': ['encoder_layer_11', 0.023251064505466896],\n",
       " 'vit_l_16_random': ['encoder', 0.028131129684027246],\n",
       " 'vit_l_32_random': ['encoder_layer_18', 0.02303210515796545],\n",
       " 'wide_resnet101_2_random': ['maxpool', 0.021429423633988885],\n",
       " 'wide_resnet50_2_random': ['maxpool', 0.021758168120326403],\n",
       " 'human_1': ['FFA-2', 0.30121278109143973],\n",
       " 'human_2': ['FFA-2', 0.17901368566414774],\n",
       " 'human_3': ['FFA-2', 0.27377389844025246],\n",
       " 'human_4': ['FFA-2', 0.26492143791638706],\n",
       " 'human_5': ['FFA-2', 0.19758419684376063],\n",
       " 'human_6': ['FFA-2', 0.027822066114759146],\n",
       " 'human_7': ['FFA-2', 0.21539203815943692]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bestScores_FFA-2_both.pkl', 'rb') as f:\n",
    "    saved_scores = pickle.load(f)\n",
    "\n",
    "saved_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in saved_scores.keys():\n",
    "    scores.append(saved_scores[model][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of models')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores[:79],20)\n",
    "plt.hist(scores[79:-7],20)\n",
    "plt.plot( [scores[-1], scores[-1]],[0, 20])\n",
    "plt.plot( [scores[-2], scores[-2]],[0, 20])\n",
    "plt.plot( [scores[-3], scores[-3]],[0, 20])\n",
    "plt.plot( [scores[-4], scores[-4]],[0, 20])\n",
    "plt.plot( [scores[-5], scores[-5]],[0, 20])\n",
    "plt.plot( [scores[-6], scores[-6]],[0, 20])\n",
    "plt.plot( [scores[-7], scores[-7]],[0, 20])\n",
    "\n",
    "\n",
    "plt.xlabel('R^2 score')\n",
    "plt.ylabel('Number of models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1554f3eaedb0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(scores),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - noise ceiling normalization\n",
    "# - normalize by how well humans predict each other "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
