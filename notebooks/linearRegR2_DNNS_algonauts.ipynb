{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 1. IMPORT LIBRARIES & SET GLOBAL VARS #\n",
    "#########################################\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# import gdown\n",
    "\n",
    "# Threshold used for selecting reliable voxels.\n",
    "NCSNR_THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import RidgeCV  # using RidgeCV with a fixed alpha\n",
    "from sklearn.metrics import r2_score as r2_score_sklearn\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Algonauts data\n",
    "\n",
    "with open('../algonauts_brain_data_joint_images_8subjects.pkl', 'rb') as f:\n",
    "    brainData = pickle.load(f)\n",
    "\n",
    "shared_images = np.load('../algonauts_joint_images_8subjects.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'maxvit_t',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_models = models.list_models(module=torchvision.models)\n",
    "avail_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_large done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "convnext_tiny done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet169 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "conv0\n",
      "norm0\n",
      "relu0\n",
      "pool0\n",
      "denseblock1\n",
      "transition1\n",
      "denseblock2\n",
      "transition2\n",
      "denseblock3\n",
      "transition3\n",
      "denseblock4\n",
      "norm5\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "densenet201 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_b7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_l done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_m done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "efficientnet_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool1\n",
      "conv2\n",
      "conv3\n",
      "maxpool2\n",
      "inception3a\n",
      "inception3b\n",
      "maxpool3\n",
      "inception4a\n",
      "inception4b\n",
      "inception4c\n",
      "inception4d\n",
      "inception4e\n",
      "maxpool4\n",
      "inception5a\n",
      "inception5b\n",
      "aux1\n",
      "aux2\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "googlenet done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "Conv2d_1a_3x3\n",
      "Conv2d_2a_3x3\n",
      "Conv2d_2b_3x3\n",
      "maxpool1\n",
      "Conv2d_3b_1x1\n",
      "Conv2d_4a_3x3\n",
      "maxpool2\n",
      "Mixed_5b\n",
      "Mixed_5c\n",
      "Mixed_5d\n",
      "Mixed_6a\n",
      "Mixed_6b\n",
      "Mixed_6c\n",
      "Mixed_6d\n",
      "Mixed_6e\n",
      "AuxLogits\n",
      "Mixed_7a\n",
      "Mixed_7b\n",
      "Mixed_7c\n",
      "avgpool\n",
      "dropout\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "inception_v3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /dev/shm/nix-build-py-torch-2.2.2.drv-0/nixbld1/spack-stage-py-torch-2.2.2-mrga1lajkybghn1l6sc83lyspvbakfzk/spack-src/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "blocks\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "maxvit_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet0_75 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "layers\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mnasnet1_3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v2 done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_large done\n",
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "mobilenet_v3_small done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_400mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_x_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_128gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_16gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_1_6gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_32gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_3_2gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_400mf done\n",
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_800mf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "stem\n",
      "trunk_output\n",
      "avgpool\n",
      "fc\n",
      "block1\n",
      "block2\n",
      "block3\n",
      "block4\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "regnet_y_8gf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet34 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnet50 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_32x8d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext101_64x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "resnext50_32x4d done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x0_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_0 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x1_5 done\n",
      "Using cuda for inference\n",
      "conv1\n",
      "maxpool\n",
      "stage2\n",
      "stage3\n",
      "stage4\n",
      "conv5\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "shufflenet_v2_x2_0 done\n",
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "squeezenet1_1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_b done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_s done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "norm\n",
      "permute\n",
      "avgpool\n",
      "flatten\n",
      "head\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "swin_v2_t done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg11_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg13_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg16_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vgg19_bn done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_b_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv_proj\n",
      "encoder\n",
      "heads\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "vit_l_32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet101_2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/home/sharvey/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n",
      "This model does not provide a transforms() method for preprocessing.  Using default.\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "wide_resnet50_2 done\n"
     ]
    }
   ],
   "source": [
    "# Extract representations resulting from probe inputs image_data for a list of models, and save them individually (takes a long time)\n",
    "\n",
    "import extract_internal_reps\n",
    "\n",
    "# internal_reps = []\n",
    "# model_2nds = []\n",
    "repDict = {}\n",
    "\n",
    "model_names = avail_models[2:] #[\"alexnet\"]\n",
    "weights = 'random' #'first'\n",
    "# image_data = test_image_data[0:1000,:,:,:]\n",
    "image_data = shared_images\n",
    "batch_size = 32\n",
    "\n",
    "for model in model_names:  #avail_models:\n",
    "    repDict = {}\n",
    "    if model == 'vit_h_14':\n",
    "        continue\n",
    "    else:\n",
    "        repDict[model + '_random'] = extract_internal_reps.get_model_activations(model, weights, image_data, batch_size=32, saverep = True, filename = 'algonauts_shared_images_random_weights')\n",
    "        print(model + \" done\")\n",
    "\n",
    "    del repDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "densenet121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../reps/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_algonauts_shared_images.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 11\u001b[0m         new_reps \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         repDict\u001b[38;5;241m.\u001b[39mupdate(new_reps)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(model_name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load some saved representations\n",
    "\n",
    "repDict = {}\n",
    "\n",
    "model_names = [\"alexnet\", \"resnet50\", \"vit_b_16\"] #avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "for model_name in model_names:\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(model_name)\n",
    "\n",
    "# model_names, _ = zip(*repDict.items())\n",
    "model_names = [value for value in repDict.keys()]\n",
    "# internal_reps = [value[0] for value in repDict.values()]\n",
    "\n",
    "\n",
    "layer_names = []\n",
    "internal_reps = []\n",
    "layer_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose subject and target brain region\n",
    "\n",
    "\n",
    "# areas = ['lh_FFA-1', 'rh_FFA-1', 'lh_FFA-2', 'rh_FFA-2']\n",
    "# areas = ['lh_all-faces', 'rh_all-faces']\n",
    "# areas = ['lh_all-words', 'rh_all-words']\n",
    "# areas = [\"lh_V1v\", \"rh_V1v\", \"lh_V1d\", \"rh_V1d\"]\n",
    "# areas = ['lh_OWFA', 'rh_OWFA']\n",
    "# areas_both_hemi = [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"] \n",
    "\n",
    "subjind = 0\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"FFA-1\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "brain_target = brainData[0][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    brain_target = np.append(brain_target, brainData[0][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(872, 541875),\n",
       " (872, 193600),\n",
       " (872, 193600),\n",
       " (872, 46656),\n",
       " (872, 139968),\n",
       " (872, 139968),\n",
       " (872, 32448),\n",
       " (872, 64896),\n",
       " (872, 64896),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 43264),\n",
       " (872, 9216),\n",
       " (872, 9216),\n",
       " (872, 9216),\n",
       " (872, 1000),\n",
       " (872, 541875),\n",
       " (872, 802816),\n",
       " (872, 802816),\n",
       " (872, 802816),\n",
       " (872, 200704),\n",
       " (872, 802816),\n",
       " (872, 401408),\n",
       " (872, 200704),\n",
       " (872, 100352),\n",
       " (872, 2048),\n",
       " (872, 1000),\n",
       " (872, 541875),\n",
       " (872, 150528),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 151296),\n",
       " (872, 1000)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure representations are flattened\n",
    "\n",
    "internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "\n",
    "[internal_rep.shape for internal_rep in internal_reps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and test sets\n",
    "\n",
    "internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "brain_target_train = brain_target[0:436]\n",
    "brain_target_test = brain_target[436:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ridge regression and compute R^2 score for the particular model layers and particular brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "resnet50\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "vit_b_16\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "\n",
    "# clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexnet': ['feature.12', 0.17273218158595455],\n",
       " 'resnet50': ['layer3', 0.1730730631140099],\n",
       " 'vit_b_16': ['encoder', 0.14308709373932468]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a9de1160>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjind = 0\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"V1v\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "brain_target = brainData[subjind][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    brain_target = np.append(brain_target, brainData[subjind][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjind = 7\n",
    "hemisphere = \"both\"  # lh, rh, or both\n",
    "areas_to_append = [\"V1v\"] \n",
    "\n",
    "\n",
    "areas = []\n",
    "extra_brain = []\n",
    "for area in areas_to_append:\n",
    "    if hemisphere == \"both\":\n",
    "        areas.append('lh_' + area)\n",
    "        areas.append('rh_' + area)\n",
    "    elif hemisphere == \"lh\":\n",
    "        areas.append('lh_' + area)\n",
    "    elif hemisphere == \"rh\":\n",
    "        areas.append('rh_' + area)\n",
    "\n",
    "# all_areas = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\", \"all-prf-visual\", \"all-bodies\", \"all-faces\", \"all-places\", \"all-words\", \"all-streams\"]\n",
    "\n",
    "extra_brain = brainData[subjind][areas[0]]\n",
    "for area in areas[1:]:\n",
    "    extra_brain = np.append(extra_brain, brainData[subjind][area],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestScores = {}\n",
    "# with open('bestScores_V1v_both.pkl', 'wb') as f:\n",
    "#     pickle.dump(bestScores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_7 prepped\n",
      "0\n",
      "subject_7 done\n"
     ]
    }
   ],
   "source": [
    "# A better way\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = ['subject_7'] #avail_models\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "#     with open('../reps/' + model_name + '_algonauts_shared_images.pkl', 'rb') as f:\n",
    "#         new_reps = pickle.load(f)\n",
    "#         repDict.update(new_reps)\n",
    "#         print(\"loaded \" + model_name)\n",
    "    repDict[model_name] = {\"V1v\": extra_brain}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open('bestScores_V1v_both.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open('bestScores_V1v_both.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded wide_resnet101_2\n",
      "wide_resnet101_2 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "wide_resnet101_2 done\n",
      "loaded wide_resnet50_2\n",
      "wide_resnet50_2 prepped\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "wide_resnet50_2 done\n"
     ]
    }
   ],
   "source": [
    "# A better way\n",
    "\n",
    "# Load a saved representation\n",
    "\n",
    "model_names = avail_models[-2:]\n",
    "# model_names.remove(\"vit_h_14\") #[\"alexnet\", \"resnet50\", \"vit_b_16\"]\n",
    "\n",
    "N_models = len(model_names)\n",
    "\n",
    "alphas = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6, 1e7, 1e7]\n",
    "\n",
    "bestScores_update = {}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    repDict = {}\n",
    "    with open('../reps/' + model_name + '_algonauts_shared_images_random_weights.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(\"loaded \" + model_name)\n",
    "\n",
    "    # model_names, _ = zip(*repDict.items())\n",
    "    # model_names = [value for value in repDict.keys()]\n",
    "    # internal_reps = [value[0] for value in repDict.values()]\n",
    "    \n",
    "    \n",
    "    layer_names = []\n",
    "    internal_reps = []\n",
    "    layer_models = []\n",
    "    \n",
    "    # for model_name in model_names:\n",
    "    layer_names.extend([value for value in repDict[model_name].keys()])\n",
    "    internal_reps.extend([value for value in repDict[model_name].values()])\n",
    "    layer_models.extend([model_name] * len([value for value in repDict[model_name].keys()]))\n",
    "\n",
    "    \n",
    "    # Make sure representations are flattened\n",
    "    \n",
    "    internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "    \n",
    "    [internal_rep.shape for internal_rep in internal_reps]\n",
    "\n",
    "    # Separate into train and test sets\n",
    "\n",
    "    internal_reps_train = [internal_rep[0:436] for internal_rep in internal_reps]\n",
    "    internal_reps_test = [internal_rep[436:] for internal_rep in internal_reps]\n",
    "    brain_target_train = brain_target[0:436]\n",
    "    brain_target_test = brain_target[436:]\n",
    "\n",
    "    print(model_name + \" prepped\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(internal_reps)):\n",
    "\n",
    "        if layer_models[i] == model_name:\n",
    "\n",
    "            Xtrain = internal_reps_train[i]\n",
    "            ytrain = brain_target_train\n",
    "\n",
    "            Xtest = internal_reps_test[i]\n",
    "            ytest = brain_target_test\n",
    "\n",
    "            clf = RidgeCV(alphas=alphas).fit(Xtrain, ytrain)\n",
    "            scores.append(clf.score(Xtest, ytest))\n",
    "            print(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    bestind = int(np.argmax(scores))\n",
    "\n",
    "    bestScores_update[model_name + '_random'] = [list(repDict[model_name].keys())[bestind], scores[bestind] ]\n",
    "\n",
    "    with open('bestScores_V1v_both.pkl', 'rb') as f:\n",
    "        bestScores = pickle.load(f)\n",
    "        bestScores.update(bestScores_update)\n",
    "\n",
    "    with open('bestScores_V1v_both.pkl', 'wb') as f:\n",
    "        pickle.dump(bestScores, f)\n",
    "\n",
    "    print(model_name + \" done\")\n",
    "    \n",
    "    del repDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexnet': ['feature.2', 0.21081742671515036],\n",
       " 'convnext_base': ['feature.1', 0.24414798025258616],\n",
       " 'convnext_large': ['feature.1', 0.2272029323199508],\n",
       " 'convnext_small': ['feature.3', 0.22214211570437653],\n",
       " 'convnext_tiny': ['feature.3', 0.22743995722533336],\n",
       " 'densenet121': ['feature.transition1', 0.23541096918901686],\n",
       " 'densenet161': ['feature.transition1', 0.2416425824103204],\n",
       " 'densenet169': ['feature.transition1', 0.2385454497335901],\n",
       " 'densenet201': ['feature.transition1', 0.2352373121453848],\n",
       " 'efficientnet_b0': ['feature.3', 0.22602563362471528],\n",
       " 'efficientnet_b1': ['feature.3', 0.22637058095146448],\n",
       " 'efficientnet_b2': ['feature.3', 0.2164318608024809],\n",
       " 'efficientnet_b3': ['feature.4', 0.2144370420098996],\n",
       " 'efficientnet_b4': ['feature.4', 0.22019426849335919],\n",
       " 'efficientnet_b5': ['feature.4', 0.21416203389122876],\n",
       " 'efficientnet_b6': ['feature.4', 0.20600278486745868],\n",
       " 'efficientnet_b7': ['feature.4', 0.2004906189480244],\n",
       " 'efficientnet_v2_l': ['feature.4', 0.2164607195985761],\n",
       " 'efficientnet_v2_m': ['feature.4', 0.21009618532432958],\n",
       " 'efficientnet_v2_s': ['feature.4', 0.21987569929974002],\n",
       " 'googlenet': ['maxpool2', 0.2546556024495357],\n",
       " 'inception_v3': ['maxpool2', 0.2602535515730982],\n",
       " 'maxvit_t': ['block.0', 0.24237548649267301],\n",
       " 'mnasnet0_5': ['layer.8', 0.21983716916385465],\n",
       " 'mnasnet0_75': ['layer.10', 0.20472781616197797],\n",
       " 'mnasnet1_0': ['layer.8', 0.21241014550646084],\n",
       " 'mnasnet1_3': ['layer.10', 0.20439948450595488],\n",
       " 'mobilenet_v2': ['feature.4', 0.24042451239390678],\n",
       " 'mobilenet_v3_large': ['feature.5', 0.24180559240713678],\n",
       " 'mobilenet_v3_small': ['feature.4', 0.24209934403965938],\n",
       " 'regnet_x_16gf': ['block2', 0.2197737417698208],\n",
       " 'regnet_x_1_6gf': ['block2', 0.22914386191951625],\n",
       " 'regnet_x_32gf': ['block2', 0.21691457251854193],\n",
       " 'regnet_x_3_2gf': ['block2', 0.22419721365928397],\n",
       " 'regnet_x_400mf': ['block2', 0.23672778667869462],\n",
       " 'regnet_x_800mf': ['block2', 0.22263888269341375],\n",
       " 'regnet_x_8gf': ['block2', 0.22215100641171998],\n",
       " 'regnet_y_128gf': ['block2', 0.18472604329490502],\n",
       " 'regnet_y_16gf': ['block2', 0.21969279820303902],\n",
       " 'regnet_y_1_6gf': ['block2', 0.22275771737388544],\n",
       " 'regnet_y_32gf': ['block2', 0.21782543822832992],\n",
       " 'regnet_y_3_2gf': ['block1', 0.22007477090163777],\n",
       " 'regnet_y_400mf': ['block1', 0.23239720830555505],\n",
       " 'regnet_y_800mf': ['block2', 0.2305421023706523],\n",
       " 'regnet_y_8gf': ['block2', 0.21397543367531616],\n",
       " 'resnet101': ['layer1', 0.24295655106601466],\n",
       " 'resnet152': ['layer2', 0.23554864335600745],\n",
       " 'resnet18': ['layer2', 0.22450670743347892],\n",
       " 'resnet34': ['maxpool', 0.21962281164201017],\n",
       " 'resnet50': ['layer2', 0.2399679946276437],\n",
       " 'resnext101_32x8d': ['layer1', 0.23189351185973583],\n",
       " 'resnext101_64x4d': ['maxpool', 0.22046670909337002],\n",
       " 'resnext50_32x4d': ['layer1', 0.22214400425619601],\n",
       " 'shufflenet_v2_x0_5': ['stage3', 0.24651406226233008],\n",
       " 'shufflenet_v2_x1_0': ['stage3', 0.23042914185498997],\n",
       " 'shufflenet_v2_x1_5': ['stage2', 0.223152471246986],\n",
       " 'shufflenet_v2_x2_0': ['stage2', 0.2224342154232895],\n",
       " 'squeezenet1_0': ['feature.9', 0.2376695584674079],\n",
       " 'squeezenet1_1': ['feature.8', 0.2530574667230755],\n",
       " 'swin_b': ['feature.3', 0.2508207264029726],\n",
       " 'swin_s': ['feature.3', 0.2512791908894579],\n",
       " 'swin_t': ['feature.3', 0.26308484927454945],\n",
       " 'swin_v2_b': ['feature.4', 0.24208775484001344],\n",
       " 'swin_v2_s': ['feature.4', 0.24283698278066473],\n",
       " 'swin_v2_t': ['feature.2', 0.23559740657051256],\n",
       " 'vgg11': ['feature.10', 0.23623445920679448],\n",
       " 'vgg11_bn': ['feature.15', 0.23427633577220153],\n",
       " 'vgg13': ['feature.9', 0.23689038746765612],\n",
       " 'vgg13_bn': ['feature.21', 0.22964626296715676],\n",
       " 'vgg16': ['feature.9', 0.24019307572654008],\n",
       " 'vgg16_bn': ['feature.14', 0.24181048929510965],\n",
       " 'vgg19': ['feature.9', 0.23717746974155984],\n",
       " 'vgg19_bn': ['feature.13', 0.239006078620093],\n",
       " 'vit_b_16': ['encoder_layer_2', 0.24282787541206938],\n",
       " 'vit_b_32': ['encoder_layer_2', 0.23293334734942703],\n",
       " 'vit_l_16': ['encoder_layer_2', 0.250241261307722],\n",
       " 'vit_l_32': ['encoder_layer_1', 0.2296631324890566],\n",
       " 'wide_resnet101_2': ['layer2', 0.23818692263213237],\n",
       " 'wide_resnet50_2': ['layer2', 0.23311964932978535],\n",
       " 'alexnet_random': ['feature.5', 0.19036650764015897],\n",
       " 'convnext_base_random': ['feature.6', 0.07369958931177244],\n",
       " 'convnext_large_random': ['feature.2', 0.07109919930610516],\n",
       " 'convnext_small_random': ['feature.5', 0.07780486737464859],\n",
       " 'convnext_tiny_random': ['feature.3', 0.07642431807252824],\n",
       " 'densenet121_random': ['feature.denseblock4', 0.15867291345007922],\n",
       " 'densenet161_random': ['feature.denseblock4', 0.13598528550840833],\n",
       " 'densenet169_random': ['feature.pool0', 0.1301100356812101],\n",
       " 'densenet201_random': ['feature.transition1', 0.15926431309061048],\n",
       " 'efficientnet_b0_random': ['feature.0', 0.08763161740383904],\n",
       " 'efficientnet_b1_random': ['feature.0', 0.08433124427493326],\n",
       " 'efficientnet_b2_random': ['feature.0', 0.08483892801086768],\n",
       " 'efficientnet_b3_random': ['feature.0', 0.08070661783054554],\n",
       " 'efficientnet_b4_random': ['feature.0', 0.08111675313831268],\n",
       " 'efficientnet_b5_random': ['feature.0', 0.0832470706886107],\n",
       " 'efficientnet_b6_random': ['feature.0', 0.08683809672800132],\n",
       " 'efficientnet_b7_random': ['feature.0', 0.08500503425964381],\n",
       " 'efficientnet_v2_l_random': ['feature.0', 0.08570504395128341],\n",
       " 'efficientnet_v2_m_random': ['feature.0', 0.07503993152875456],\n",
       " 'efficientnet_v2_s_random': ['feature.0', 0.08464064725319449],\n",
       " 'googlenet_random': ['maxpool2', 0.17438228827342367],\n",
       " 'inception_v3_random': ['Mixed_5c', 0.12944452675598614],\n",
       " 'maxvit_t_random': ['block.0', 0.07999364891565851],\n",
       " 'mnasnet0_5_random': ['layer.11', 0.11952570497438483],\n",
       " 'mnasnet0_75_random': ['layer.11', 0.09762908201071509],\n",
       " 'mnasnet1_0_random': ['layer.11', 0.08428443279798267],\n",
       " 'mnasnet1_3_random': ['layer.1', 0.09083724059740884],\n",
       " 'mobilenet_v2_random': ['feature.13', 0.1332353242553934],\n",
       " 'mobilenet_v3_large_random': ['feature.2', 0.07803888188389503],\n",
       " 'mobilenet_v3_small_random': ['feature.7', 0.09350432736130801],\n",
       " 'regnet_x_16gf_random': ['stem', 0.087091592338192],\n",
       " 'regnet_x_1_6gf_random': ['stem', 0.0803193338285667],\n",
       " 'regnet_x_32gf_random': ['stem', 0.08090009952476926],\n",
       " 'regnet_x_3_2gf_random': ['stem', 0.08661294910999837],\n",
       " 'regnet_x_400mf_random': ['block1', 0.09018628369659673],\n",
       " 'regnet_x_800mf_random': ['stem', 0.08963320987108382],\n",
       " 'regnet_x_8gf_random': ['stem', 0.08611453336738067],\n",
       " 'regnet_y_128gf_random': ['stem', 0.08091990929136197],\n",
       " 'regnet_y_16gf_random': ['stem', 0.08672275882582484],\n",
       " 'regnet_y_1_6gf_random': ['stem', 0.0903075979549402],\n",
       " 'regnet_y_32gf_random': ['stem', 0.0919435412289435],\n",
       " 'regnet_y_3_2gf_random': ['stem', 0.08694487294393859],\n",
       " 'regnet_y_400mf_random': ['stem', 0.08173272237968657],\n",
       " 'regnet_y_800mf_random': ['stem', 0.08550103630492971],\n",
       " 'regnet_y_8gf_random': ['stem', 0.07903804940059676],\n",
       " 'resnet101_random': ['maxpool', 0.1586697582703687],\n",
       " 'resnet152_random': ['maxpool', 0.15976577636006417],\n",
       " 'resnet18_random': ['maxpool', 0.15240297720297225],\n",
       " 'resnet34_random': ['maxpool', 0.14914701979503175],\n",
       " 'resnet50_random': ['maxpool', 0.1489571904235467],\n",
       " 'resnext101_32x8d_random': ['maxpool', 0.15781435242612474],\n",
       " 'resnext101_64x4d_random': ['layer1', 0.16167539915595427],\n",
       " 'resnext50_32x4d_random': ['maxpool', 0.15442832284775765],\n",
       " 'shufflenet_v2_x0_5_random': ['maxpool', 0.12184574675108213],\n",
       " 'shufflenet_v2_x1_0_random': ['maxpool', 0.13473989398971611],\n",
       " 'shufflenet_v2_x1_5_random': ['maxpool', 0.1340991331458906],\n",
       " 'shufflenet_v2_x2_0_random': ['maxpool', 0.12820454636014755],\n",
       " 'squeezenet1_0_random': ['feature.6', 0.15818920452658694],\n",
       " 'squeezenet1_1_random': ['feature.5', 0.19463025436726206],\n",
       " 'swin_b_random': ['feature.7', 0.08940853223530047],\n",
       " 'swin_s_random': ['feature.7', 0.08214483127077328],\n",
       " 'swin_t_random': ['feature.7', 0.08273292080036757],\n",
       " 'swin_v2_b_random': ['feature.3', 0.09980858265400921],\n",
       " 'swin_v2_s_random': ['feature.3', 0.09921361378483834],\n",
       " 'swin_v2_t_random': ['feature.3', 0.08740740375396987],\n",
       " 'vgg11_random': ['feature.20', 0.1828014974149401],\n",
       " 'vgg11_bn_random': ['feature.28', 0.16184238202379791],\n",
       " 'vgg13_random': ['feature.24', 0.17228309542260656],\n",
       " 'vgg13_bn_random': ['feature.32', 0.1992933125448897],\n",
       " 'vgg16_random': ['feature.28', 0.1913373321140763],\n",
       " 'vgg16_bn_random': ['feature.38', 0.1685715404107215],\n",
       " 'vgg19_random': ['feature.34', 0.17975963623688138],\n",
       " 'vgg19_bn_random': ['feature.50', 0.18600456055083966],\n",
       " 'vit_b_16_random': ['encoder_layer_11', 0.09357349929872746],\n",
       " 'vit_b_32_random': ['encoder', 0.09030777673245942],\n",
       " 'vit_l_16_random': ['encoder_layer_15', 0.09442021220300245],\n",
       " 'vit_l_32_random': ['encoder_layer_23', 0.09338942321783301],\n",
       " 'wide_resnet101_2_random': ['maxpool', 0.13949041059714615],\n",
       " 'wide_resnet50_2_random': ['maxpool', 0.14525376009051671],\n",
       " 'subject_1': ['V1v', 0.2693175350901412],\n",
       " 'subject_2': ['V1v', 0.18598661733980112],\n",
       " 'subject_3': ['V1v', 0.14070160700390155],\n",
       " 'subject_4': ['V1v', 0.11620465329050328],\n",
       " 'subject_5': ['V1v', 0.14007313976335278],\n",
       " 'subject_6': ['V1v', 0.16790330291802555],\n",
       " 'subject_7': ['V1v', 0.1474784688341643]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bestScores_V1v_both.pkl', 'rb') as f:\n",
    "    saved_scores = pickle.load(f)\n",
    "\n",
    "saved_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in saved_scores.keys():\n",
    "    scores.append(saved_scores[model][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of models')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores[:79],20)\n",
    "plt.hist(scores[79:-7],20)\n",
    "plt.plot( [scores[-1], scores[-1]],[0, 20])\n",
    "plt.plot( [scores[-2], scores[-2]],[0, 20])\n",
    "plt.plot( [scores[-3], scores[-3]],[0, 20])\n",
    "plt.plot( [scores[-4], scores[-4]],[0, 20])\n",
    "plt.plot( [scores[-5], scores[-5]],[0, 20])\n",
    "plt.plot( [scores[-6], scores[-6]],[0, 20])\n",
    "plt.plot( [scores[-7], scores[-7]],[0, 20])\n",
    "\n",
    "\n",
    "plt.xlabel('R^2 score')\n",
    "plt.ylabel('Number of models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1554f3ccd010>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(scores),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# - noise ceiling normalization\n",
    "# - normalize by how well humans predict each other "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
