{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max and Min metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 1. IMPORT LIBRARIES & SET GLOBAL VARS #\n",
    "#########################################\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import gdown\n",
    "\n",
    "# Threshold used for selecting reliable voxels.\n",
    "NCSNR_THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 2. HELPER FUNCTIONS                   #\n",
    "#########################################\n",
    "\n",
    "def r2_over_nc(y, y_pred, ncsnr):\n",
    "    \"\"\"\n",
    "    Compute the R^2 score normalized by the noise ceiling (NC) as in Finzi et al (2022).\n",
    "    The noise ceiling is computed as:\n",
    "         NC = ncsnr^2 / (ncsnr^2 + 1/num_trials)\n",
    "    If ncsnr is None, return the standard R^2.\n",
    "    \"\"\"\n",
    "    ### TODO: Replace the code below with your implementation.\n",
    "    # Instructions:\n",
    "    # 1. If ncsnr is None, compute and return the standard R^2 score using\n",
    "    #    r2_score_sklearn (with multioutput=\"raw_values\").\n",
    "    # 2. Otherwise, assume there are 3 target trials (i.e. set num_trials = 3.0).\n",
    "    # 3. Compute the noise ceiling (NC) using the formula:\n",
    "    #       NC = (ncsnr ** 2) / ( (ncsnr ** 2) + (1.0 / num_trials) )\n",
    "    # 4. Compute the standard R^2 score (using r2_score_sklearn) and then\n",
    "    #    return the normalized R^2 score by dividing the R^2 score by NC.\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metadata_concat_hemi(Y):\n",
    "    \"\"\"\n",
    "    Concatenate left- and right-hemisphere metadata for voxels labeled 'nsdgeneral'\n",
    "    and return the corresponding ncsnr values and metadata DataFrame.\n",
    "    \"\"\"\n",
    "    ncsnr_full = np.concatenate((\n",
    "        Y['voxel_metadata']['lh']['lh.ncsnr'],\n",
    "        Y['voxel_metadata']['rh']['rh.ncsnr']\n",
    "    ))\n",
    "    \n",
    "    nsdgeneral_idx = np.concatenate((\n",
    "        Y['voxel_metadata']['lh']['lh.nsdgeneral.label'],\n",
    "        Y['voxel_metadata']['rh']['rh.nsdgeneral.label']\n",
    "    ))\n",
    "    nsdgeneral_mask = np.logical_and(nsdgeneral_idx == 'nsdgeneral', ncsnr_full > 0)\n",
    "    ncsnr_nsdgeneral = ncsnr_full[nsdgeneral_mask]\n",
    "    \n",
    "    metadata_lh = pd.DataFrame(Y['voxel_metadata']['lh'])\n",
    "    metadata_rh = pd.DataFrame(Y['voxel_metadata']['rh'])\n",
    "    nsdgeneral_metadata_df = pd.concat([metadata_lh, metadata_rh])[nsdgeneral_mask]\n",
    "    \n",
    "    return ncsnr_nsdgeneral, nsdgeneral_metadata_df\n",
    "\n",
    "\n",
    "def get_data_dict(Y, brain_data_rep_averaged, ncsnr_nsdgeneral, nsdgeneral_metadata_df, verbose=True):\n",
    "    \"\"\"\n",
    "    For each brain area (both streams and visual ROIs), select voxels with reliable responses\n",
    "    (ncsnr above threshold) and return a dictionary with responses and ncsnr values.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    # Process streams-based areas.\n",
    "    for area in ['ventral', 'parietal', 'lateral']:\n",
    "        data_dict[area] = {}\n",
    "        lh_area_mask = nsdgeneral_metadata_df['lh.streams.label'].astype(str).str.contains(area, na=False)\n",
    "        rh_area_mask = nsdgeneral_metadata_df['rh.streams.label'].astype(str).str.contains(area, na=False)\n",
    "        area_mask = np.logical_or(lh_area_mask, rh_area_mask)\n",
    "        area_mask = np.logical_and(area_mask, ncsnr_nsdgeneral > NCSNR_THRESHOLD)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Size of area {area}: {np.sum(area_mask)}\")\n",
    "        \n",
    "        area_data = brain_data_rep_averaged[:, area_mask]\n",
    "        data_dict[area][\"responses\"] = area_data.copy()\n",
    "        data_dict[area][\"ncsnr\"] = ncsnr_nsdgeneral[area_mask].copy()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Shape of area {area} responses: {data_dict[area]['responses'].shape}\")\n",
    "\n",
    "    # Process visual ROIs.\n",
    "    for area in ['V1', 'V2', 'V3', 'V4']:\n",
    "        data_dict[area] = {}\n",
    "        lh_area_mask = nsdgeneral_metadata_df['lh.prf-visualrois.label'].astype(str).str.contains(area, na=False)\n",
    "        rh_area_mask = nsdgeneral_metadata_df['rh.prf-visualrois.label'].astype(str).str.contains(area, na=False)\n",
    "        area_mask = np.logical_or(lh_area_mask, rh_area_mask)\n",
    "        area_mask = np.logical_and(area_mask, ncsnr_nsdgeneral > NCSNR_THRESHOLD)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Size of area {area}: {np.sum(area_mask)}\")\n",
    "        \n",
    "        area_data = brain_data_rep_averaged[:, area_mask]\n",
    "        data_dict[area][\"responses\"] = area_data.copy()\n",
    "        data_dict[area][\"ncsnr\"] = ncsnr_nsdgeneral[area_mask].copy()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Shape of area {area} responses: {data_dict[area]['responses'].shape}\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in Y: dict_keys(['image_data', 'brain_data', 'voxel_metadata', 'image_metadata'])\n",
      "Shape of image brick (train): (1000, 425, 425, 3)\n",
      "Shape of image brick (val): (1000, 425, 425, 3)\n",
      "Shape of image brick (test): (1000, 425, 425, 3)\n",
      "Keys in Y: dict_keys(['image_data', 'brain_data', 'voxel_metadata', 'image_metadata'])\n",
      "Shape of image brick (train): (1000, 425, 425, 3)\n",
      "Shape of image brick (val): (1000, 425, 425, 3)\n",
      "Shape of image brick (test): (1000, 425, 425, 3)\n",
      "Keys in Y: dict_keys(['image_data', 'brain_data', 'voxel_metadata', 'image_metadata'])\n",
      "Shape of image brick (train): (1000, 425, 425, 3)\n",
      "Shape of image brick (val): (1000, 425, 425, 3)\n",
      "Shape of image brick (test): (1000, 425, 425, 3)\n",
      "Keys in Y: dict_keys(['image_data', 'brain_data', 'voxel_metadata', 'image_metadata'])\n",
      "Shape of image brick (train): (1000, 425, 425, 3)\n",
      "Shape of image brick (val): (1000, 425, 425, 3)\n",
      "Shape of image brick (test): (1000, 425, 425, 3)\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# 3. DOWNLOAD & LOAD NSD DATA           #\n",
    "#########################################\n",
    "\n",
    "# Create a data directory if it does not exist.\n",
    "datadir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "subjects = ['subj01', 'subj02', 'subj05', 'subj07']\n",
    "\n",
    "Ydict = {}\n",
    "\n",
    "for subj in subjects:\n",
    "\n",
    "    # Define subject and corresponding file_id.\n",
    "    overwrite = False\n",
    "\n",
    "    if subj == 'subj01':\n",
    "        file_id = '13cRiwhjurCdr4G2omRZSOMO_tmatjdQr'\n",
    "    elif subj == 'subj02':\n",
    "        file_id = '1MO9reLoV4fqu6Weh4gmE78KJVtxg72ID'\n",
    "    elif subj == 'subj05':\n",
    "        file_id = '11dPt3Llj6eAEDJnaRy8Ch5CxfeKijX_t'\n",
    "    elif subj == 'subj07':\n",
    "        file_id = '1HX-6t4c6js6J_vP4Xo0h1fbK2WINpwem'\n",
    "        \n",
    "    url = f'https://drive.google.com/uc?id={file_id}&export=download'\n",
    "    output = os.path.join(datadir, f'{subj}_nativesurface_nsdgeneral.pkl')\n",
    "\n",
    "    if not exists(output) or overwrite:\n",
    "        gdown.download(url, output, quiet=False)\n",
    "\n",
    "    # Load NSD data.\n",
    "    Y = np.load(output, allow_pickle=True)\n",
    "    print(\"Keys in Y:\", Y.keys())\n",
    "\n",
    "    Ydict[subj] = Y\n",
    "\n",
    "\n",
    "    # Print shapes of image bricks for each partition.\n",
    "    for partition in ['train', 'val', 'test']:\n",
    "        print(f\"Shape of image brick ({partition}):\", Y['image_data'][partition].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj01', 'subj02', 'subj05', 'subj07']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453622,) 0.2057 0.1881\n",
      "['n/a' 'nsdgeneral']\n",
      "69609\n",
      "(69556,) 0.5064 0.2213\n",
      "Size of area ventral: 18378\n",
      "Shape of area ventral responses: (2000, 18378)\n",
      "Size of area parietal: 10377\n",
      "Shape of area parietal responses: (2000, 10377)\n",
      "Size of area lateral: 13388\n",
      "Shape of area lateral responses: (2000, 13388)\n",
      "Size of area V1: 6999\n",
      "Shape of area V1 responses: (2000, 6999)\n",
      "Size of area V2: 6806\n",
      "Shape of area V2 responses: (2000, 6806)\n",
      "Size of area V3: 5438\n",
      "Shape of area V3 responses: (2000, 5438)\n",
      "Size of area V4: 2497\n",
      "Shape of area V4 responses: (2000, 2497)\n",
      "Size of area ventral: 18378\n",
      "Shape of area ventral responses: (1000, 18378)\n",
      "Size of area parietal: 10377\n",
      "Shape of area parietal responses: (1000, 10377)\n",
      "Size of area lateral: 13388\n",
      "Shape of area lateral responses: (1000, 13388)\n",
      "Size of area V1: 6999\n",
      "Shape of area V1 responses: (1000, 6999)\n",
      "Size of area V2: 6806\n",
      "Shape of area V2 responses: (1000, 6806)\n",
      "Size of area V3: 5438\n",
      "Shape of area V3 responses: (1000, 5438)\n",
      "Size of area V4: 2497\n",
      "Shape of area V4 responses: (1000, 2497)\n",
      "(478942,) 0.2048 0.1999\n",
      "['n/a' 'nsdgeneral']\n",
      "71419\n",
      "(71229,) 0.5245 0.2632\n",
      "Size of area ventral: 18778\n",
      "Shape of area ventral responses: (2000, 18778)\n",
      "Size of area parietal: 10633\n",
      "Shape of area parietal responses: (2000, 10633)\n",
      "Size of area lateral: 13367\n",
      "Shape of area lateral responses: (2000, 13367)\n",
      "Size of area V1: 6417\n",
      "Shape of area V1 responses: (2000, 6417)\n",
      "Size of area V2: 5527\n",
      "Shape of area V2 responses: (2000, 5527)\n",
      "Size of area V3: 4837\n",
      "Shape of area V3 responses: (2000, 4837)\n",
      "Size of area V4: 2138\n",
      "Shape of area V4 responses: (2000, 2138)\n",
      "Size of area ventral: 18778\n",
      "Shape of area ventral responses: (1000, 18778)\n",
      "Size of area parietal: 10633\n",
      "Shape of area parietal responses: (1000, 10633)\n",
      "Size of area lateral: 13367\n",
      "Shape of area lateral responses: (1000, 13367)\n",
      "Size of area V1: 6417\n",
      "Shape of area V1 responses: (1000, 6417)\n",
      "Size of area V2: 5527\n",
      "Shape of area V2 responses: (1000, 5527)\n",
      "Size of area V3: 4837\n",
      "Shape of area V3 responses: (1000, 4837)\n",
      "Size of area V4: 2138\n",
      "Shape of area V4 responses: (1000, 2138)\n",
      "(396502,) 0.2109 0.2033\n",
      "['n/a' 'nsdgeneral']\n",
      "59736\n",
      "(59639,) 0.5533 0.2268\n",
      "Size of area ventral: 16551\n",
      "Shape of area ventral responses: (2000, 16551)\n",
      "Size of area parietal: 8006\n",
      "Shape of area parietal responses: (2000, 8006)\n",
      "Size of area lateral: 11846\n",
      "Shape of area lateral responses: (2000, 11846)\n",
      "Size of area V1: 5585\n",
      "Shape of area V1 responses: (2000, 5585)\n",
      "Size of area V2: 5267\n",
      "Shape of area V2 responses: (2000, 5267)\n",
      "Size of area V3: 4219\n",
      "Shape of area V3 responses: (2000, 4219)\n",
      "Size of area V4: 2063\n",
      "Shape of area V4 responses: (2000, 2063)\n",
      "Size of area ventral: 16551\n",
      "Shape of area ventral responses: (1000, 16551)\n",
      "Size of area parietal: 8006\n",
      "Shape of area parietal responses: (1000, 8006)\n",
      "Size of area lateral: 11846\n",
      "Shape of area lateral responses: (1000, 11846)\n",
      "Size of area V1: 5585\n",
      "Shape of area V1 responses: (1000, 5585)\n",
      "Size of area V2: 5267\n",
      "Shape of area V2 responses: (1000, 5267)\n",
      "Size of area V3: 4219\n",
      "Shape of area V3 responses: (1000, 4219)\n",
      "Size of area V4: 2063\n",
      "Shape of area V4 responses: (1000, 2063)\n",
      "(399162,) 0.1585 0.1418\n",
      "['n/a' 'nsdgeneral']\n",
      "60623\n",
      "(60522,) 0.3783 0.1603\n",
      "Size of area ventral: 14544\n",
      "Shape of area ventral responses: (2000, 14544)\n",
      "Size of area parietal: 7065\n",
      "Shape of area parietal responses: (2000, 7065)\n",
      "Size of area lateral: 12814\n",
      "Shape of area lateral responses: (2000, 12814)\n",
      "Size of area V1: 5493\n",
      "Shape of area V1 responses: (2000, 5493)\n",
      "Size of area V2: 4733\n",
      "Shape of area V2 responses: (2000, 4733)\n",
      "Size of area V3: 3781\n",
      "Shape of area V3 responses: (2000, 3781)\n",
      "Size of area V4: 1397\n",
      "Shape of area V4 responses: (2000, 1397)\n",
      "Size of area ventral: 14544\n",
      "Shape of area ventral responses: (1000, 14544)\n",
      "Size of area parietal: 7065\n",
      "Shape of area parietal responses: (1000, 7065)\n",
      "Size of area lateral: 12814\n",
      "Shape of area lateral responses: (1000, 12814)\n",
      "Size of area V1: 5493\n",
      "Shape of area V1 responses: (1000, 5493)\n",
      "Size of area V2: 4733\n",
      "Shape of area V2 responses: (1000, 4733)\n",
      "Size of area V3: 3781\n",
      "Shape of area V3 responses: (1000, 3781)\n",
      "Size of area V4: 1397\n",
      "Shape of area V4 responses: (1000, 1397)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################\n",
    "# 5. PREPARE FMRI DATA                  #\n",
    "#########################################\n",
    "human_data = []\n",
    "\n",
    "for i in range(4):\n",
    "    Y = Ydict[subjects[i]]\n",
    "    # Concatenate full brain ncsnr and nsdgeneral labels.\n",
    "    ncsnr_full = np.concatenate((\n",
    "        Y['voxel_metadata']['lh']['lh.ncsnr'].values,\n",
    "        Y['voxel_metadata']['rh']['rh.ncsnr'].values\n",
    "    ))\n",
    "    nsdgeneral_idx = np.concatenate((\n",
    "        Y['voxel_metadata']['lh']['lh.nsdgeneral.label'].values,\n",
    "        Y['voxel_metadata']['rh']['rh.nsdgeneral.label'].values\n",
    "    ))\n",
    "    print(ncsnr_full.shape, round(np.mean(ncsnr_full), 4), round(np.std(ncsnr_full), 4))\n",
    "    print(np.unique(nsdgeneral_idx))\n",
    "    print(np.count_nonzero(nsdgeneral_idx == 'nsdgeneral'))\n",
    "    \n",
    "    # Select only nsdgeneral voxels with positive ncsnr.\n",
    "    nsdgeneral_mask = np.logical_and(nsdgeneral_idx == 'nsdgeneral', ncsnr_full > 0)\n",
    "    ncsnr_nsdgeneral = ncsnr_full[nsdgeneral_mask]\n",
    "    print(ncsnr_nsdgeneral.shape, round(np.mean(ncsnr_nsdgeneral), 4), round(np.std(ncsnr_nsdgeneral), 4))\n",
    "    \n",
    "    # Combine metadata for nsdgeneral voxels.\n",
    "    nsdgeneral_metadata = pd.concat((\n",
    "        Y['voxel_metadata']['lh'],\n",
    "        Y['voxel_metadata']['rh']\n",
    "    ))[nsdgeneral_mask]\n",
    "    ncsnr_nsdgeneral, nsdgeneral_metadata_df = get_metadata_concat_hemi(Y)\n",
    "    \n",
    "    # Concatenate train and validation brain data and average over repetitions.\n",
    "    train_brain_data_cat = np.concatenate((\n",
    "        Y['brain_data']['train']['lh'],\n",
    "        Y['brain_data']['train']['rh']\n",
    "    ), axis=2)\n",
    "    val_brain_data_cat = np.concatenate((\n",
    "        Y['brain_data']['val']['lh'],\n",
    "        Y['brain_data']['val']['rh']\n",
    "    ), axis=2)\n",
    "    train_brain_data_cat = np.concatenate((train_brain_data_cat, val_brain_data_cat), axis=0)\n",
    "    train_brain_data_cat = np.mean(train_brain_data_cat, axis=1)\n",
    "    \n",
    "    # Average test brain data over repetitions.\n",
    "    test_brain_data_cat = np.concatenate((\n",
    "        Y['brain_data']['test']['lh'],\n",
    "        Y['brain_data']['test']['rh']\n",
    "    ), axis=2)\n",
    "    test_brain_data_cat = np.mean(test_brain_data_cat, axis=1)\n",
    "    \n",
    "    # Get fMRI data dictionaries for train and test sets.\n",
    "    train_fmri_data = get_data_dict(Y, train_brain_data_cat, ncsnr_nsdgeneral, nsdgeneral_metadata_df)\n",
    "    test_fmri_data = get_data_dict(Y, test_brain_data_cat, ncsnr_nsdgeneral, nsdgeneral_metadata_df)\n",
    "    \n",
    "    # Use both train and validation images for training.\n",
    "    train_image_data = np.concatenate((Y['image_data']['train'], Y['image_data']['val']), axis=0)\n",
    "    test_image_data = Y['image_data']['test']\n",
    "    \n",
    "    # Define a torchvision transform: resize, center crop, convert to tensor, and normalize.\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    human_data.append(test_fmri_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'maxvit_t',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_models = models.list_models(module=torchvision.models)\n",
    "avail_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efficientnet_v2_l'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete some models that we don't want \n",
    "model_names = [x for x in avail_models if not (x.startswith('regnet') or x.startswith('wide') or x.startswith('squeeze') )]\n",
    "model_names = model_names[0:-4]\n",
    "model_names.pop(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract representations resulting from probe inputs image_data\n",
    "\n",
    "import extract_internal_reps\n",
    "\n",
    "# data_dir = '../DeepNSD/neural_data/natural_scenes_demo/stimulus_set_700'\n",
    "save_dir = os.getcwd()\n",
    "\n",
    "# internal_reps = []\n",
    "# model_2nds = []\n",
    "repDict = {}\n",
    "\n",
    "for model in model_names:  #avail_models:\n",
    "    repDict.clear()\n",
    "    x1, model_2nd = extract_internal_reps.extract_rep_nsd(model, test_image_data, weights=\"random\")\n",
    "    repDict[model] = [x1,model_2nd]\n",
    "    # model_2nds.append(model_2nd)\n",
    "    # internal_reps.append(x1)\n",
    "    with open(save_dir + '/reps/' + model + '_internal_rep_classifier_COCO_1000_aran_random_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(repDict, f)\n",
    "    print(model + \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "densenet121\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b2\n",
      "efficientnet_b3\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "efficientnet_v2_m\n",
      "efficientnet_v2_s\n",
      "googlenet\n",
      "inception_v3\n",
      "maxvit_t\n",
      "mnasnet0_5\n",
      "mnasnet0_75\n",
      "mnasnet1_0\n",
      "mnasnet1_3\n",
      "mobilenet_v2\n",
      "mobilenet_v3_large\n",
      "mobilenet_v3_small\n",
      "resnet101\n",
      "resnet152\n",
      "resnet18\n",
      "resnet34\n",
      "resnet50\n",
      "resnext101_32x8d\n",
      "resnext101_64x4d\n",
      "resnext50_32x4d\n",
      "shufflenet_v2_x0_5\n",
      "shufflenet_v2_x1_0\n",
      "shufflenet_v2_x1_5\n",
      "shufflenet_v2_x2_0\n",
      "swin_b\n",
      "swin_s\n",
      "swin_t\n",
      "swin_v2_b\n",
      "swin_v2_s\n",
      "swin_v2_t\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "vit_b_16\n"
     ]
    }
   ],
   "source": [
    "# Load a specified set of internal reps in a dictionary \n",
    "\n",
    "repDict = {}\n",
    "\n",
    "N_models = len(model_names)\n",
    "for model_name in model_names:\n",
    "    with open('reps/' + model_name + '_internal_rep_classifier_COCO_1000_aran_first_weights.pkl', 'rb') as f:\n",
    "        new_reps = pickle.load(f)\n",
    "        repDict.update(new_reps)\n",
    "        print(model_name)\n",
    "\n",
    "# model_names, _ = zip(*repDict.items())\n",
    "model_names = [value for value in repDict.keys()]\n",
    "internal_reps = [value[0] for value in repDict.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names, _ = zip(*repDict.items())\n",
    "model_names = [value for value in repDict.keys()]\n",
    "internal_reps = [value[0] for value in repDict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append some brain data to the list of representations\n",
    "\n",
    "# areas = ['V1', 'V2', 'V3', 'V4']\n",
    "areas = ['V4']\n",
    "\n",
    "\n",
    "for area in areas:\n",
    "    internal_reps.append(human_data[0][area][\"responses\"])\n",
    "    model_names.append(\"Subject_1_\" + '_' + area)\n",
    "\n",
    "for area in areas:\n",
    "    internal_reps.append(human_data[1][area][\"responses\"])\n",
    "    model_names.append(\"Subject_2_\" + '_' + area)\n",
    "\n",
    "for area in areas:\n",
    "    internal_reps.append(human_data[2][area][\"responses\"])\n",
    "    model_names.append(\"Subject_5_\" + '_' + area)\n",
    "\n",
    "for area in areas:\n",
    "    internal_reps.append(human_data[3][area][\"responses\"])\n",
    "    model_names.append(\"Subject_7_\" + '_' + area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure representations are flattened\n",
    "\n",
    "internal_reps = [internal_reps[i].reshape((internal_reps[i].shape[0], np.prod(list(internal_reps[i].shape[1:])) )) for i in range(len(internal_reps))]\n",
    "\n",
    "[internal_rep.shape for internal_rep in internal_reps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute/load normalized kernels for all of the reps\n",
    "\n",
    "dd = metrics.LinearDecodingSimilarityMulti(center_columns=True, a = 0, b = 1)\n",
    "# cached = dd.cache(internal_reps) # this part takes a long time\n",
    "cached = list(np.load('Kx_cache_a0b1_56models_4subjects_V4only.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KX = cached[56]\n",
    "KY = cached[3]\n",
    "A = (1/2)*(KX@KY + KY@KX)\n",
    "\n",
    "vals, vecs = np.linalg.eig(A)\n",
    "\n",
    "idx = vals.argsort()[::-1]   \n",
    "vals = vals[idx]\n",
    "vecs = vecs[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal eigenvalue z\n",
    "\n",
    "zind = 0\n",
    "sorted_img_idx = vecs[:,zind].argsort()[::-1]\n",
    "plt.plot(np.sort(vecs[:,zind]),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nimgs = 50\n",
    "ncols = 8\n",
    "nrows = int(np.ceil(nimgs/ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12,200))\n",
    "ind = 0\n",
    "\n",
    "for row in range(nrows):\n",
    "    for column in range(ncols):\n",
    "        if ind == nimgs:\n",
    "            break\n",
    "        else:\n",
    "            image = Y1[\"image_data\"][\"test\"][sorted_img_idx[ind],:,:,:]\n",
    "            ax = axes[row, column]\n",
    "            # ax.set_title(f\"Image ({row}, {column})\")\n",
    "            ax.axis('off')\n",
    "            ax.imshow(image)\n",
    "            ind += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=False, tight_layout=True)\n",
    "\n",
    "n_bins = 30\n",
    "\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "axs[0].hist(vecs[0], bins=n_bins)\n",
    "axs[1].hist(vecs[-1], bins=n_bins)\n",
    "# axs[2].hist(vals, bins=n_bins)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
