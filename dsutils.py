"""
Decoding similarity helper functions.
"""

import torch
import warnings
import numpy as np
import numpy.typing as npt
from typing import Tuple


from torch import nn
# from torchvision import models

class Flatten(nn.Module):
    def __init__(self):
        super(Flatten, self).__init__()
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        return x

class SoftMaxModule(nn.Module):
    def __init__(self):
        super(SoftMaxModule, self).__init__()
        self.softmax = nn.Softmax()

    def forward(self, x):
        return self.softmax(x)


def bespoke_cov_matrix(z):
    """
    Computes the covariance matrix for a custom set of tasks.  Assumes tasks are equally weighted.  

    Parameters
    ----------
    z : list of tasks.  Each element of list should be an array of (number of samples x 1).  These are the desired readouts for every input sample.  

    Returns
    -------
    Cz : ndarray
        M x M empirical covariance matrix.
    """

    n = len(z)

    Cz = 0
    for i in range(n):
        Cz = Cz + z[i]@z[i].T
    Cz = Cz/(n)

    return Cz




def random_partitions_cov_matrix(M, n):
    """
    Computes the empirical task covariance matrix for a set of n random binary partitions of the input samples.  Readouts are sampled from Rademacher distribution for each input sample. 

    Parameters
    ----------
    M : int (number of input samples)
    n : int (number of random tasks to generate)

    Returns
    -------
    Cz : ndarray
        M x M empirical covariance matrix.
    """
    z = []
    for i in range(n):
        zrand = np.random.randint(0,2,(M,1)) 
        zrand = 2*zrand - 1
        z.append(zrand)

    Cz = 0
    for i in range(n):
        Cz = Cz + z[i]@z[i].T
    Cz = Cz/(n)

    return Cz


def gaussian_partitions_cov_matrix(M, n):
    """
    Computes the empirical task covariance matrix if the desired readouts are a set of n samples from the M-dimensional standard normal distribution. 

    Parameters
    ----------
    M : int (number of input samples)
    n : int (number of random tasks to generate)

    Returns
    -------
    Cz : ndarray
        M x M empirical covariance matrix.
    """
    z = []
    for i in range(n):
        zrand = np.random.normal(0,1,(M,1)) 
        z.append(zrand)

    Cz = 0
    for i in range(n):
        Cz = Cz + z[i]@z[i].T
    Cz = Cz/(n)

    return Cz


def update_gaussian_cov_matrix(Cz, n, delta_n):
    """ Update a Gaussian covariance matrix generated by gaussian_partitions_cov_matrix(M, n) with delta_n more samples.  
    """
    M = np.shape(Cz)[0]
    new_cov_contrib = delta_n*gaussian_partitions_cov_matrix(M,delta_n)
    newCz = (1/(n + delta_n))*(n*Cz + new_cov_contrib)
    return newCz

def update_random_cov_matrix(Cz, n, delta_n):
    """ Update a random binary partitions covariance matrix generated by random_partitions_cov_matrix(M, n) with delta_n more samples.  
    """
    M = np.shape(Cz)[0]
    new_cov_contrib = delta_n*random_partitions_cov_matrix(M,delta_n)
    newCz = (1/(n + delta_n))*(n*Cz + new_cov_contrib)
    return newCz


class PartitionsCovMatrix:
    """
    Make task covariance matrices to evaluate average decoding similarity.  
    """
    def __init__(self, M, n_initial, method = 'binary'):
        self.M = M
        self.n = n_initial
        self.method = method
        self.matrix = None

    def initialize_cov_matrix(self):
        if self.method == 'binary':
            Cz = random_partitions_cov_matrix(self.M, self.n)
        elif self.method == 'gaussian':
            Cz = gaussian_partitions_cov_matrix(self.M, self.n)
        else:
            raise ValueError(
                "method must be either 'binary' or 'gaussian'.")
        self.matrix = Cz
        
        return None

    def update_cov_matrix(self, add_n):
        if self.method == 'binary':
            Cz = update_random_cov_matrix(self.matrix, self.n, add_n)
        elif self.method == 'gaussian':
            Cz = update_gaussian_cov_matrix(self.matrix, self.n, add_n)
        else:
            raise ValueError(
                "method must be either 'binary' or 'gaussian'.")
        
        self.n = self.n + add_n
        self.matrix = Cz
        
        return None


def whiten(
    X: npt.NDArray, 
    alpha: float, 
    preserve_variance: bool = True, 
    eigval_tol=1e-7
    ) -> Tuple[npt.NDArray, npt.NDArray]:
    """Return regularized whitening transform for a matrix X.

    Parameters
    ----------
    X : ndarray
        Matrix with shape `(m, n)` holding `m` observations
        in `n`-dimensional feature space. Columns of `X` are
        expected to be mean-centered so that `X.T @ X` is
        the covariance matrix.
    alpha : float
        Regularization parameter, `0 <= alpha <= 1`. When
        `alpha == 0`, the data matrix is fully whitened.
        When `alpha == 1` the data matrix is not transformed
        (`Z == eye(X.shape[1])`).
    preserve_variance : bool
        If True, rescale the (partial) whitening matrix so
        that the total variance, trace(X.T @ X), is preserved.
    eigval_tol : float
        Eigenvalues of covariance matrix are clipped to this
        minimum value.

    Returns
    -------
    X_whitened : ndarray
        Transformed data matrix.
    Z : ndarray
        Matrix implementing the whitening transformation.
        `X_whitened = X @ Z`.
    """

    # Return early if regularization is maximal (no whitening).
    if alpha > (1 - eigval_tol):
        return X, np.eye(X.shape[1])

    # Compute eigendecomposition of covariance matrix
    lam, V = np.linalg.eigh(X.T @ X)
    lam = np.maximum(lam, eigval_tol)

    # Compute diagonal of (partial) whitening matrix.
    # 
    # When (alpha == 1), then (d == ones).
    # When (alpha == 0), then (d == 1 / sqrt(lam)).
    d = alpha + (1 - alpha) * lam ** (-1 / 2)

    # Rescale the whitening matrix.
    if preserve_variance:

        # Compute the variance of the transformed data.
        #
        # When (alpha == 1), then new_var = sum(lam)
        # When (alpha == 0), then new_var = len(lam)
        new_var = np.sum(
            (alpha ** 2) * lam
            + 2 * alpha * (1 - alpha) * (lam ** 0.5)
            + ((1 - alpha) ** 2) * np.ones_like(lam)
        )

        # Now re-scale d so that the variance of (X @ Z)
        # will equal the original variance of X.
        d *= np.sqrt(np.sum(lam) / new_var)

    # Form (partial) whitening matrix.
    Z = (V * d[None, :]) @ V.T

    # An alternative regularization strategy would be:
    #
    # lam, V = np.linalg.eigh(X.T @ X)
    # d = lam ** (-(1 - alpha) / 2)
    # Z = (V * d[None, :]) @ V.T

    # Returned (partially) whitened data and whitening matrix.
    return X @ Z, Z